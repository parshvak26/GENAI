{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJTrPjQa2YGbqdIhlJb9iT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parshvak26/GENAI/blob/main/Complete_GenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0E1pICdJNQM8"
      },
      "outputs": [],
      "source": [
        "# !pip install -U google-genai>=1.37.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "e48hX8yFO-lM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)\n"
      ],
      "metadata": {
        "id": "xc5s28y-PDYx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyASAH0l-sxogbn5lSZIQ07ei9YaM0MJAx8\"\n"
      ],
      "metadata": {
        "id": "w4MeSpouPKAX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain AI to me like I'm a kid.\"\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpTAA3w2PnZJ",
        "outputId": "2bd9dfcd-0c7a-486f-ea90-9067b1ac5003"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine you have a **super-duper smart friend** who lives inside computers and robots!\n",
            "\n",
            "This friend is called **AI** (which stands for **Artificial Intelligence**).\n",
            "\n",
            "Here's what makes this friend special:\n",
            "\n",
            "1.  **It can learn!** Just like you learn new things by looking at pictures, listening to your teacher, or trying things out, AI learns too. It learns by looking at *tons* of examples. If you show it a million pictures of cats, it learns what a cat looks like!\n",
            "\n",
            "2.  **It can think (a little)!** Not exactly like your brain, but it can make smart guesses or decisions based on what it's learned.\n",
            "\n",
            "3.  **It's a helper!** AI's main job is to help us do cool things.\n",
            "\n",
            "**Here are some places you might already see your AI friend:**\n",
            "\n",
            "*   **When you talk to your tablet or smart speaker** (like Siri or Alexa): That's AI understanding your words and finding answers.\n",
            "*   **When a website suggests a video or game you might like:** That's AI remembering what you watched before and guessing what else you'd enjoy.\n",
            "*   **When your phone unlocks just by looking at your face:** That's AI recognizing your face!\n",
            "*   **In some video games:** The computer characters you play against or with might be using AI to act smart.\n",
            "*   **Robot vacuums:** They use AI to figure out where to clean and how to avoid bumping into things.\n",
            "\n",
            "So, AI is like a really clever digital helper that learns from a lot of information to do awesome, useful things for us! It doesn't have feelings or get tired like you, but it's super good at following smart rules and learning patterns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "rudKuPzMSWMF",
        "outputId": "ff8f77c5-4e94-4633-f565-688e55361b2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, imagine you have a **super-duper smart friend** who lives inside computers and robots!\n\nThis friend is called **AI** (which stands for **Artificial Intelligence**).\n\nHere's what makes this friend special:\n\n1.  **It can learn!** Just like you learn new things by looking at pictures, listening to your teacher, or trying things out, AI learns too. It learns by looking at *tons* of examples. If you show it a million pictures of cats, it learns what a cat looks like!\n\n2.  **It can think (a little)!** Not exactly like your brain, but it can make smart guesses or decisions based on what it's learned.\n\n3.  **It's a helper!** AI's main job is to help us do cool things.\n\n**Here are some places you might already see your AI friend:**\n\n*   **When you talk to your tablet or smart speaker** (like Siri or Alexa): That's AI understanding your words and finding answers.\n*   **When a website suggests a video or game you might like:** That's AI remembering what you watched before and guessing what else you'd enjoy.\n*   **When your phone unlocks just by looking at your face:** That's AI recognizing your face!\n*   **In some video games:** The computer characters you play against or with might be using AI to act smart.\n*   **Robot vacuums:** They use AI to figure out where to clean and how to avoid bumping into things.\n\nSo, AI is like a really clever digital helper that learns from a lot of information to do awesome, useful things for us! It doesn't have feelings or get tired like you, but it's super good at following smart rules and learning patterns."
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
        "response = chat.send_message('Hello! My name is Parshva. I am just starting with GENAI. Any tips?')\n"
      ],
      "metadata": {
        "id": "l5o5QhTLScN4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g1sXvAS6TwsZ",
        "outputId": "8ccf2b4b-0fdf-437d-bb78-be87771f185f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi Parshva, welcome to the world of Generative AI (GenAI)! It's a fascinating and rapidly evolving field. Here's a breakdown of tips to help you get started:\n\n**1. Foundational Knowledge: Build a Solid Base**\n\n*   **Understand Core Concepts:**\n    *   **AI vs. Machine Learning vs. Deep Learning vs. GenAI:** Know the relationships between these terms. AI is the broad field, ML is a subset, Deep Learning is a subset of ML, and GenAI is a specific type of AI/ML model.\n    *   **Neural Networks:**  Learn the basics of how neural networks work. You don't need to be a mathematician, but understanding layers, activation functions, and how they learn is helpful.\n    *   **Large Language Models (LLMs):** These are the workhorses behind many GenAI applications. Familiarize yourself with their architecture, training process, and limitations. Common types include Transformers (the foundation of models like GPT).\n    *   **Generative Models:** Understand how these models *generate* new data (text, images, audio, etc.) instead of just classifying or predicting. Concepts like probability distributions and sampling are relevant.\n    *   **Tokenization:** How text is broken down into units for processing by LLMs.\n    *   **Prompt Engineering:**  The art of crafting effective prompts to get the desired output from GenAI models.\n    *   **Evaluation Metrics:** How to measure the quality of generated content (e.g., perplexity, BLEU score, ROUGE score).\n*   **Key Terminology:**\n    *   **Parameters:**  The adjustable weights in a model.  More parameters often (but not always) means a more powerful model.\n    *   **Training Data:** The data used to teach the model. The quality and quantity of training data are crucial.\n    *   **Fine-tuning:** Adapting a pre-trained model to a specific task or dataset.\n    *   **Inference:** The process of using a trained model to generate new outputs.\n    *   **Hallucinations:** When a GenAI model confidently produces incorrect or nonsensical information.\n    *   **Bias:** When the model exhibits unfair or discriminatory behavior due to biases in the training data.\n\n**2.  Learning Resources: Where to Learn**\n\n*   **Online Courses:**\n    *   **Coursera & edX:** Look for courses on Machine Learning, Deep Learning, and Natural Language Processing. Some specifically focus on GenAI. Andrew Ng's Machine Learning course is a classic.  DeepLearning.AI offers specialized courses on LLMs.\n    *   **Fast.ai:** Practical, code-first approach to deep learning.\n    *   **Udemy:** A wide range of courses, often at affordable prices.\n    *   **Google's Machine Learning Crash Course:** A free, accessible introduction to ML.\n    *   **Hugging Face:** They offer fantastic free courses on Transformers and other aspects of NLP.  Check out the \"Hugging Face Course.\"\n*   **Books:**\n    *   \"Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\" by Aurélien Géron (a good general ML/DL book).\n    *   \"Deep Learning\" by Goodfellow, Bengio, and Courville (more theoretical).\n    *   Look for books specifically on Generative AI as they are published (it's a hot topic!).\n*   **Blogs & Websites:**\n    *   **Hugging Face Blog:**  Stay up-to-date on the latest GenAI research and tools.\n    *   **Google AI Blog:**  Technical blog from Google's AI researchers.\n    *   **OpenAI Blog:**  Updates on OpenAI's models and research.\n    *   **Towards Data Science (Medium):**  A wide variety of articles on data science and AI.\n    *   **Papers with Code:**  A great resource for finding and understanding research papers.\n*   **Research Papers:**\n    *   Read foundational papers on Transformer architecture (e.g., \"Attention is All You Need\").\n    *   Explore papers on specific generative models like GANs, VAEs, and diffusion models.\n*   **YouTube Channels:**\n    *   **Sentdex:** Python programming and machine learning tutorials.\n    *   **3Blue1Brown:**  Excellent visualizations of mathematical concepts related to neural networks.\n    *   **Lex Fridman Podcast:** Interviews with leading AI researchers and practitioners.\n    *   **Two Minute Papers:**  Concise summaries of recent research papers.\n\n**3.  Hands-on Practice: Code and Experiment**\n\n*   **Choose a Programming Language:** Python is the most popular language for AI/ML due to its extensive libraries.\n*   **Install Libraries:**\n    *   **TensorFlow and/or PyTorch:** Deep learning frameworks. TensorFlow is generally considered easier to start with, but PyTorch is gaining popularity.\n    *   **Transformers (Hugging Face):** A library for working with pre-trained Transformer models.  Essential for many GenAI tasks.\n    *   **Scikit-learn:** A general-purpose machine learning library (useful for data preprocessing and evaluation).\n    *   **Other helpful libraries:** NumPy (numerical computation), Pandas (data manipulation), Matplotlib/Seaborn (visualization).\n*   **Start with Simple Projects:**\n    *   **Text Generation:** Use a pre-trained language model (e.g., from Hugging Face) to generate text based on a given prompt.\n    *   **Image Generation:** Experiment with image generation models like DALL-E mini or Stable Diffusion (using readily available APIs or libraries).\n    *   **Fine-tuning a Model:** Take a pre-trained model and fine-tune it on a specific dataset (e.g., a dataset of product reviews).\n    *   **Chatbot:** Build a simple chatbot using a language model.\n*   **Use Cloud Platforms:**\n    *   **Google Colab:** Free cloud-based environment with pre-installed ML libraries and GPU/TPU access.\n    *   **Kaggle:** Offers a free cloud environment and access to datasets and competitions.\n    *   **AWS SageMaker, Google Cloud AI Platform, Azure Machine Learning:** More advanced cloud platforms for building and deploying ML models.\n\n**4.  Prompt Engineering: The Art of Guiding GenAI**\n\n*   **Learn the Principles:**\n    *   **Be Clear and Specific:** Avoid ambiguous or vague prompts.\n    *   **Provide Context:** Give the model enough information to understand the task.\n    *   **Use Keywords:** Include relevant keywords to guide the model's output.\n    *   **Specify the Desired Output Format:** Tell the model what type of output you want (e.g., a poem, a summary, a list).\n    *   **Use Examples:**  Provide examples of the desired output to help the model learn. This is called \"few-shot learning.\"\n    *   **Iterate and Refine:** Experiment with different prompts and refine them based on the results you get.\n*   **Explore Prompt Engineering Techniques:**\n    *   **Chain-of-Thought Prompting:**  Encourage the model to break down a problem into smaller steps.\n    *   **Zero-Shot, One-Shot, Few-Shot Learning:** Experiment with providing different numbers of examples in your prompts.\n    *   **Role-Playing:**  Ask the model to take on a specific persona.\n*   **Resources for Prompt Engineering:**\n    *   Learn Prompting (website)\n    *   Various blog posts and articles on prompt engineering.\n\n**5.  Ethical Considerations: Be Responsible**\n\n*   **Understand Potential Biases:** Be aware that GenAI models can perpetuate biases present in their training data.\n*   **Address Hallucinations:**  Implement strategies to detect and mitigate hallucinations.\n*   **Consider the Impact:** Think about the potential societal impact of your GenAI applications (e.g., misinformation, job displacement).\n*   **Explore Responsible AI Frameworks:** Familiarize yourself with frameworks like Google's AI Principles and Microsoft's Responsible AI Standard.\n*   **Data Privacy:** Adhere to data privacy regulations (e.g., GDPR) when using personal data.\n\n**6.  Stay Up-to-Date: The Field is Moving Fast**\n\n*   **Follow Researchers and Industry Leaders:** Track the work of leading researchers and companies in the field.\n*   **Attend Conferences and Workshops:**  Stay informed about the latest advancements and network with other practitioners.\n*   **Read Research Papers Regularly:**  Keep up with the latest research on GenAI models and techniques.\n*   **Join Online Communities:**  Engage with other GenAI enthusiasts on forums, Slack channels, and social media.\n\n**7.  Specific Tips for You, Parshva:**\n\n*   **Start Small:** Don't try to learn everything at once. Focus on a specific area of GenAI that interests you (e.g., text generation, image generation, music generation).\n*   **Don't Be Afraid to Experiment:**  Try different things and see what works.\n*   **Ask Questions:**  Don't hesitate to ask questions in online forums or to your mentors.\n*   **Build a Portfolio:** Create projects that showcase your skills and understanding of GenAI.  This will be valuable when looking for jobs or collaborating with others.\n*   **Network:**  Connect with other people in the field.  Attend meetups, join online communities, and reach out to people whose work you admire.\n*   **Be Patient:** Learning GenAI takes time and effort. Don't get discouraged if you don't understand everything right away.\n\n**Key Takeaway:**\n\nThe best way to learn GenAI is by doing. Start with the fundamentals, find a project that excites you, and start coding.  The field is rapidly changing, so be prepared to learn continuously. Good luck!\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHf8fArfT0vd",
        "outputId": "373e283f-aeab-4b52-ad66-1d5d5b0fc6ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-preview\n",
            "models/veo-3.0-fast-generate-preview\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n",
            "models/gemini-2.5-flash-live-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == 'models/gemini-2.5-flash':\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5JQSqdeT6kf",
        "outputId": "ab63d8cf-c309-478e-8521-7806b1568447"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Stable version of Gemini 2.5 Flash, our mid-size multimodal '\n",
            "                'model that supports up to 1 million tokens, released in June '\n",
            "                'of 2025.',\n",
            " 'display_name': 'Gemini 2.5 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.5-flash',\n",
            " 'output_token_limit': 65536,\n",
            " 'supported_actions': ['generateContent',\n",
            "                       'countTokens',\n",
            "                       'createCachedContent',\n",
            "                       'batchGenerateContent'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '001'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Temperature**\n",
        "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
        "\n",
        "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
      ],
      "metadata": {
        "id": "181T6C1uUrNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
        "# high_temp_config = types.GenerateContentConfig(temperature=0.0) #This is Low temperature. Try uncommenting above one and see change in output\n",
        "\n",
        "\n",
        "\n",
        "# for _ in range(5):\n",
        "#   response = client.models.generate_content(\n",
        "#       model='gemini-2.5-flash',\n",
        "#       config=high_temp_config,\n",
        "#       contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "#   if response.text:\n",
        "#     print(response.text, '-' * 25)"
      ],
      "metadata": {
        "id": "h3t2uUf8T_LZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Top-P**\n",
        "Like temperature, the top-P parameter is also used to control the diversity of the model's output.\n",
        "\n",
        "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
        "\n",
        "You may also see top-K referenced in LLM literature. Top-K is not configurable in the Gemini 2.0 series of models, but can be changed in older models. Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
        "\n",
        "Run this example a number of times, change the settings and observe the change in output."
      ],
      "metadata": {
        "id": "o9Wj5K_QVIBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_config = types.GenerateContentConfig(\n",
        "#     # These are the default values for gemini-2.0-flash.\n",
        "#     temperature=1.0,\n",
        "#     top_p=0.95,\n",
        "# )\n",
        "\n",
        "# story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=model_config,\n",
        "#     contents=story_prompt)\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "rU_Lo9neVTBL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompting**"
      ],
      "metadata": {
        "id": "0vqYrp65XhDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Zero Shot**\n",
        "\n",
        "Zero-shot prompts are prompts that describe the request for the model directly.\n",
        "\n",
        "**EXAMPLE**- Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment:"
      ],
      "metadata": {
        "id": "u0ExNwrcXmOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enum mode**\n",
        "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards."
      ],
      "metadata": {
        "id": "sZc118JWYCAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM**, short for Enumeration, is basically a fancy way of saying “a list of named options you can choose from.” It’s not some secret AI spell — it’s just a data structure used in programming (and yes, also in GenAI frameworks) to define a fixed set of values that something can take.\n",
        "\n",
        "Think of it like giving names to a few specific choices instead of letting someone type random junk. It’s like saying:\n",
        "\n",
        "“You can pick from {Cat, Dog, Hamster}, but not ‘DragonWithWiFi’.”"
      ],
      "metadata": {
        "id": "Bfz7sc61adDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you’re designing a GenAI pipeline where the model can only act in certain modes:\n",
        "\n",
        "    class AgentAction(Enum):\n",
        "\n",
        "      SUMMARIZE = \"summarize\"\n",
        "\n",
        "      TRANSLATE = \"translate\"\n",
        "\n",
        "      CODE = \"code\"\n",
        "\n",
        "\n",
        "So, if the model gets “SUMMARIZE”, it knows exactly what operation to perform. Keeps things neat, predictable, and stops the AI from hallucinating another mode called “make memes”."
      ],
      "metadata": {
        "id": "0qKInIFzarv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import enum\n",
        "\n",
        "# class Sentiment(enum.Enum):\n",
        "#     POSITIVE = \"positive\"\n",
        "#     NEUTRAL = \"neutral\"\n",
        "#     NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         response_mime_type=\"text/x.enum\",\n",
        "#         response_schema=Sentiment\n",
        "#     ),\n",
        "#     contents=zero_shot_prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "### OUTPUT would be \"positive\"\n",
        "\n",
        "\n",
        "# enum_response = response.parsed\n",
        "# print(enum_response)\n",
        "# print(type(enum_response))"
      ],
      "metadata": {
        "id": "g6m1c-YZXkAH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM** = a controlled vocabulary for your GenAI system.\n",
        "It ensures structure and sanity in a world full of chaotic data and even more chaotic humans.\n",
        "\n",
        "As Seneca said, “Order is what keeps the universe from sliding into chaos.”"
      ],
      "metadata": {
        "id": "xy9VlGWia6db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One-shot and few-shot**\n",
        "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt."
      ],
      "metadata": {
        "id": "Qpnxrr_sdE8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "# customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         temperature=0.1,\n",
        "#         top_p=1,\n",
        "#         max_output_tokens=250,\n",
        "#     ),\n",
        "#     contents=[few_shot_prompt, customer_order])\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "727sAItvYMAI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chain of Thought (CoT)**\n",
        "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
        "\n",
        "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
        "\n",
        "Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try."
      ],
      "metadata": {
        "id": "VQX-v-yljc0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "# am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "# Output -\n",
        "# 52"
      ],
      "metadata": {
        "id": "h8rsJjxZjf6N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "# I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# Markdown(response.text)\n",
        "\n",
        "# Output-\n",
        "# Here's how to solve this:\n",
        "\n",
        "# Find the age difference: When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
        "\n",
        "# Calculate the age difference: The age difference between you and your partner is 12 - 4 = 8 years.\n",
        "\n",
        "# Determine partner's current age: Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
        "\n",
        "# Answer: Your partner is currently 28 years old."
      ],
      "metadata": {
        "id": "wkUWbxxrjmKC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ReAct: Reason and act**\n",
        "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper."
      ],
      "metadata": {
        "id": "hQfRkvf4kuln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of an AI model just thinking (reasoning) or just doing (acting), ReAct makes it do both — in turns.\n",
        "\n",
        "The model reasons about the problem step-by-step, takes an action (like calling a tool, searching, or retrieving info), observes the result, and then continues reasoning.\n",
        "Think of it like a detective alternating between thinking out loud and doing stuff until the mystery’s solved."
      ],
      "metadata": {
        "id": "Hi5urWTwpWnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Step-by-step*\n",
        "\n",
        "Reasoning → the model thinks: “Hmm, what do I need to solve this?”\n",
        "\n",
        "Acting → it takes an action: “Let me look that up in the database.”\n",
        "\n",
        "Observation → it gets the result: “Okay, here’s what I found.”\n",
        "\n",
        "Repeat → it continues reasoning with the new info until it reaches the answer.\n",
        "\n",
        "So instead of blindly guessing, the model becomes something like a thoughtful agent that mixes logic with action — like a data scientist who actually tests their hypothesis instead of just tweeting it."
      ],
      "metadata": {
        "id": "wCXMNdF4pu1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Question - “What’s the current temperature in New York?”\n",
        "\n",
        "    The AI’s internal process might look like this:\n",
        "\n",
        "      Thought: I need real-time weather data.\n",
        "\n",
        "      Action: call_weather_api(\"New York\")\n",
        "\n",
        "      Observation: The API returns 12°C.\n",
        "\n",
        "      Thought: The temperature in New York is 12°C.\n",
        "\n",
        "      Final Answer: It’s 12°C in New York."
      ],
      "metadata": {
        "id": "8Z8iiLlcp_gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_instructions = \"\"\"\n",
        "# Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "# Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        "#  (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "#      will return some similar entities to search and you can try to search the information from those topics.\n",
        "#  (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "#      so keep your searches short.\n",
        "#  (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "# \"\"\"\n",
        "\n",
        "# example1 = \"\"\"Question\n",
        "# Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "# Thought 1\n",
        "# The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "# Action 1\n",
        "# <search>Milhouse</search>\n",
        "# Observation 1\n",
        "# Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "# Thought 2\n",
        "# The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "# Action 2\n",
        "# <lookup>named after</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "# Thought 3\n",
        "# Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "# Action 3\n",
        "# <finish>Richard Nixon</finish>\n",
        "# \"\"\"\n",
        "\n",
        "# example2 = \"\"\"Question\n",
        "# What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "# Thought 1\n",
        "# I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "# Action 1\n",
        "# <search>Colorado orogeny</search>\n",
        "\n",
        "# Observation 1\n",
        "# The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "# Thought 2\n",
        "# It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "# Action 2\n",
        "# <lookup>eastern sector</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "# Thought 3\n",
        "# The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "# Action 3\n",
        "# <search>High Plains</search>\n",
        "\n",
        "# Observation 3\n",
        "# High Plains refers to one of two distinct land regions\n",
        "\n",
        "# Thought 4\n",
        "# I need to instead search High Plains (United States).\n",
        "\n",
        "# Action 4\n",
        "# <search>High Plains (United States)</search>\n",
        "\n",
        "# Observation 4\n",
        "# The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "# Thought 5\n",
        "# High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "# Action 5\n",
        "# <finish>1,800 to 7,000 ft</finish>\n",
        "# \"\"\"\n",
        "# # Take a look through https://github.com/ysymyth/ReAct/\n"
      ],
      "metadata": {
        "id": "eucmY6UHkxpv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"\"\"Question\n",
        "# Who was the youngest author listed on the transformers NLP paper?\n",
        "# \"\"\"\n",
        "\n",
        "# # You will perform the Action; so generate up to, but not including, the Observation.\n",
        "# react_config = types.GenerateContentConfig(\n",
        "#     stop_sequences=[\"\\nObservation\"],\n",
        "#     system_instruction=model_instructions + example1 + example2,\n",
        "# )\n",
        "\n",
        "# # Create a chat that has the model instructions and examples pre-seeded.\n",
        "# react_chat = client.chats.create(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     config=react_config,\n",
        "# )\n",
        "\n",
        "# resp = react_chat.send_message(question)\n",
        "# print(resp.text)"
      ],
      "metadata": {
        "id": "vmjC5coRlMiP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Thinking mode**\n",
        "The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n",
        "\n",
        "Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated."
      ],
      "metadata": {
        "id": "MSHrgednQP9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "# from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "# response = client.models.generate_content_stream(\n",
        "#     model='gemini-2.0-flash-thinking-exp',\n",
        "#     contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        "# )\n",
        "\n",
        "# buf = io.StringIO()\n",
        "# for chunk in response:\n",
        "#     buf.write(chunk.text)\n",
        "#     # Display the response as it is streamed\n",
        "#     print(chunk.text, end='')\n",
        "\n",
        "# # And then render the finished response as formatted markdown.\n",
        "# clear_output()\n",
        "# Markdown(buf.getvalue())"
      ],
      "metadata": {
        "id": "OF8KliWnQSyE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Q&A with RAG using Chroma\n"
      ],
      "metadata": {
        "id": "0nmEocRvRbPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two big limitations of LLMs are 1) that they only \"know\" the information that they were trained on, and 2) that they have limited input context windows. A way to address both of these limitations is to use a technique called Retrieval Augmented Generation, or RAG. A RAG system has three stages:\n",
        "\n",
        "    Indexing\n",
        "    Retrieval\n",
        "    Generation"
      ],
      "metadata": {
        "id": "a2v-GQwBRgh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Indexing → Break documents into chunks, turn them into embeddings (numerical meaning), and store them in a vector database.\n",
        "\n",
        "2. Retrieval → When asked a question, find the most relevant chunks from that database using semantic similarity.\n",
        "\n",
        "3. Generation → Feed those chunks + the question to the LLM so it can craft a grounded, natural-language answer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  - Documents → [Indexing] → Vector DB\n",
        "\n",
        "  - User Query → [Retrieval] → Top Relevant Chunks\n",
        "\n",
        "  - Query + Chunks → [Generation] → Final Answer"
      ],
      "metadata": {
        "id": "CJjWD7WUTNB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG, short for Retrieval-Augmented Generation, is just a fancy combo move that helps AI answer questions based on real documents"
      ],
      "metadata": {
        "id": "YpwpNRTASoNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal LLMs are good at language but terrible at memory.\n",
        "You ask about some obscure company policy PDF — I can’t know it, because it’s not in my training data.\n",
        "\n",
        "So, RAG fixes that by giving the model access to your specific documents.\n",
        "\n",
        "It works like this:\n",
        "\n",
        "1. Store your documents somewhere searchable (usually as chunks of text in a “vector database” — basically a big mathy filing cabinet).\n",
        "\n",
        "2. When you ask a question, the system:\n",
        "\n",
        "    - Searches for the most relevant parts of those docs.\n",
        "\n",
        "    - Pulls out the top matches.\n",
        "\n",
        "3. Then it feeds both your question and those retrieved snippets into the language model.\n",
        "\n",
        "4. The model uses that context to generate a smart, grounded answer — not a wild guess.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UN6QDvzoSQ9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "    if \"embedContent\" in m.supported_actions:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfR8i125Rdt0",
        "outputId": "aecf2c9d-93d7-4f6a-918e-a65482e1cd5e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Data"
      ],
      "metadata": {
        "id": "grN5EbpKVizu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\"\n",
        "DOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.'\n",
        "DOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"\n",
        "\n",
        "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
      ],
      "metadata": {
        "id": "D9Ri1rUdVkyi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install chromadb\n"
      ],
      "metadata": {
        "id": "CAuwwEceWG-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "# Define a helper to retry when per-minute quota is reached.\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = \"retrieval_document\"\n",
        "        else:\n",
        "            embedding_task = \"retrieval_query\"\n",
        "\n",
        "        response = client.models.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            contents=input,\n",
        "            config=types.EmbedContentConfig(\n",
        "                task_type=embedding_task,\n",
        "            ),\n",
        "        )\n",
        "        return [e.values for e in response.embeddings]"
      ],
      "metadata": {
        "id": "yCOQ3NlfVnXp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "DB_NAME = \"googlecardb\"\n",
        "\n",
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
        "\n",
        "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAZsrX2kVwlU",
        "outputId": "cf95ebae-dfe8-4e50-c201-338f0ab35e44"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4060984393.py:5: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  embed_fn = GeminiEmbeddingFunction()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9tPA-l2XFXM",
        "outputId": "a0f94567-9cc5-4cc1-82ec-ad138b1bf816"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to query mode when generating embeddings.\n",
        "embed_fn.document_mode = False\n",
        "\n",
        "# Search the Chroma DB using the specified query.\n",
        "query = \"How do you use the touchscreen to play music?\"\n",
        "\n",
        "result = db.query(query_texts=[query], n_results=1)\n",
        "[all_passages] = result[\"documents\"]\n",
        "\n",
        "Markdown(all_passages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "PCYhVsrxXKjY",
        "outputId": "b2ff36a6-04f3-4cfe-dba7-ed97c2251164"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs."
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_passages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI_KY52ba5Ud",
        "outputId": "b7d45a2a-4961-408f-91fe-0b2ce9a6c0df"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Augmented generation: Answer the question**\n",
        "\n",
        "Now that we’ve found a relevant passage from our document set during the retrieval step, we can move on to assembling a generation prompt and use the Gemini API to produce the final answer.\n",
        "\n",
        "In this example, we only retrieved a single passage. But in real-world scenarios — especially when dealing with a large collection of data — we’d typically retrieve multiple passages. That way, the Gemini model can decide which pieces of text are actually useful for answering the question.\n",
        "\n",
        "It’s perfectly fine if a few of those retrieved passages aren’t directly relevant; the model’s generation process is designed to filter out the noise and focus on what truly matters."
      ],
      "metadata": {
        "id": "4RUBd5M_aAn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_oneline = query.replace(\"\\n\", \" \")\n",
        "\n",
        "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
        "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "QUESTION: {query_oneline}\n",
        "\"\"\"\n",
        "\n",
        "# Add the retrieved documents to the prompt.\n",
        "for passage in all_passages:\n",
        "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
        "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ya2lOdKXhgB",
        "outputId": "40676cc7-012e-40aa-ccf0-84d907a03e5f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
            "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \n",
            "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
            "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
            "\n",
            "QUESTION: How do you use the touchscreen to play music?\n",
            "PASSAGE: Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(answer.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "ooOuI5F6atCi",
        "outputId": "90724827-8413-41e5-b601-678a468f4329"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "It's super easy to play music in your Googlecar! All you need to do is simply touch the \"Music\" icon on your large touchscreen display, and then you can start enjoying your favorite songs."
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVmyheG1bhgY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}