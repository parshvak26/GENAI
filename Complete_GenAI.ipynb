{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYcKZ5bpZznvxxqlNxUArP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parshvak26/GENAI/blob/main/Complete_GenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0E1pICdJNQM8"
      },
      "outputs": [],
      "source": [
        "# !pip install -U google-genai>=1.37.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "e48hX8yFO-lM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)\n"
      ],
      "metadata": {
        "id": "xc5s28y-PDYx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyASAH0l-sxogbn5lSZIQ07ei9YaM0MJAx8\"\n"
      ],
      "metadata": {
        "id": "w4MeSpouPKAX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain AI to me like I'm a kid.\"\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpTAA3w2PnZJ",
        "outputId": "eca49a68-2669-44f0-f685-bfa90ed8a64e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine your brain! Your brain helps you learn new things, remember stuff, solve puzzles, and even figure out what someone means when they're talking. It's pretty amazing, right?\n",
            "\n",
            "Well, imagine we could try to give a **computer** a super-smart \"brain\" too! That's kind of what **AI** (which stands for **Artificial Intelligence**) is.\n",
            "\n",
            "It's like giving computers special instructions and tools so they can:\n",
            "\n",
            "1.  **Learn:** Just like you learn to ride a bike by trying it many times, AI can \"learn\" by looking at tons and tons of information. If you show it 100 pictures of cats, it can learn what a cat looks like!\n",
            "2.  **Think a little bit:** Not like a real person thinks, but it can figure out answers, make decisions, or even create new things based on what it's learned.\n",
            "3.  **Help us:** AI helps grown-ups do all sorts of cool things!\n",
            "\n",
            "**Here are some examples you might know:**\n",
            "\n",
            "*   **Siri or Alexa:** When you ask them a question, and they try to understand what you said and give you an answer, that's AI!\n",
            "*   **Video Games:** Sometimes, the other players in your game who aren't real people, but computer characters, seem pretty smart and know how to play. That's AI!\n",
            "*   **Netflix or YouTube:** When they suggest another show or video you might like, they're using AI to guess what you'd enjoy based on what you've watched before.\n",
            "*   **Self-driving cars:** Cars that can *see* the road, other cars, and people, and drive themselves safely using AI.\n",
            "\n",
            "So, AI is just like giving computers a special kind of \"thinking power\" so they can help us learn, play, discover new things, and make our world even more amazing! It's like having a super-smart computer helper!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "rudKuPzMSWMF",
        "outputId": "2e50950d-6ccb-4379-e602-e453dc2a64dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, imagine your brain! Your brain helps you learn new things, remember stuff, solve puzzles, and even figure out what someone means when they're talking. It's pretty amazing, right?\n\nWell, imagine we could try to give a **computer** a super-smart \"brain\" too! That's kind of what **AI** (which stands for **Artificial Intelligence**) is.\n\nIt's like giving computers special instructions and tools so they can:\n\n1.  **Learn:** Just like you learn to ride a bike by trying it many times, AI can \"learn\" by looking at tons and tons of information. If you show it 100 pictures of cats, it can learn what a cat looks like!\n2.  **Think a little bit:** Not like a real person thinks, but it can figure out answers, make decisions, or even create new things based on what it's learned.\n3.  **Help us:** AI helps grown-ups do all sorts of cool things!\n\n**Here are some examples you might know:**\n\n*   **Siri or Alexa:** When you ask them a question, and they try to understand what you said and give you an answer, that's AI!\n*   **Video Games:** Sometimes, the other players in your game who aren't real people, but computer characters, seem pretty smart and know how to play. That's AI!\n*   **Netflix or YouTube:** When they suggest another show or video you might like, they're using AI to guess what you'd enjoy based on what you've watched before.\n*   **Self-driving cars:** Cars that can *see* the road, other cars, and people, and drive themselves safely using AI.\n\nSo, AI is just like giving computers a special kind of \"thinking power\" so they can help us learn, play, discover new things, and make our world even more amazing! It's like having a super-smart computer helper!"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
        "response = chat.send_message('Hello! My name is Parshva. I am just starting with GENAI. Any tips?')\n"
      ],
      "metadata": {
        "id": "l5o5QhTLScN4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g1sXvAS6TwsZ",
        "outputId": "e0b0e6fd-9b47-4132-861f-a46c2dbec9cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi Parshva, welcome to the exciting world of Generative AI! It's a rapidly evolving field, so buckle up and get ready to learn. Here are some tips to help you get started:\n\n**1. Build a Solid Foundation:**\n\n*   **Understand the Fundamentals:**\n    *   **Machine Learning (ML):** GenAI builds upon ML concepts. Get familiar with supervised, unsupervised, and reinforcement learning.\n    *   **Deep Learning (DL):** Dive into neural networks, especially recurrent neural networks (RNNs) and transformers, as these are key to many GenAI models.\n    *   **Natural Language Processing (NLP):** If you're interested in text-based GenAI, understanding NLP is essential. Learn about tokenization, embeddings, language models, etc.\n*   **Brush Up on Python:**\n    *   Python is the dominant programming language in the AI/ML world. Make sure you're comfortable with the basics and key libraries like NumPy, Pandas, and scikit-learn.\n*   **Mathematics is Your Friend:**\n    *   A solid understanding of linear algebra, calculus, probability, and statistics will be incredibly helpful in understanding the underlying mathematics of GenAI algorithms. Don't be intimidated; you don't need to be a math whiz, but familiarity will empower you.\n\n**2. Choose Your Focus Area (at least initially):**\n\nGenAI is broad! Choose a specific area to begin with. Some popular choices include:\n\n*   **Text Generation:**  Focus on models like GPT, LLaMA, or PaLM.\n*   **Image Generation:** Explore models like DALL-E, Stable Diffusion, or Midjourney.\n*   **Audio Generation:** Look into models that generate music, speech, or sound effects.\n*   **Code Generation:** Consider models like GitHub Copilot or Codex.\n\n**3. Get Hands-On with Practical Projects:**\n\n*   **Start Simple:** Don't try to build a cutting-edge model right away. Begin with small, manageable projects.\n*   **Use Pre-trained Models:** Leverage pre-trained models to avoid training from scratch. Hugging Face's Transformers library is a treasure trove of models.  Experiment with them!\n*   **Follow Tutorials and Courses:** There are countless online resources to guide you. Look for tutorials that walk you through building simple GenAI applications.\n\n**4. Learn About the Tools and Frameworks:**\n\n*   **TensorFlow and PyTorch:** These are the two major deep learning frameworks.  Start with one and become proficient. PyTorch is often considered more beginner-friendly.\n*   **Hugging Face Transformers:** An amazing library providing access to thousands of pre-trained models and tools for working with them.\n*   **Cloud Platforms:** Become familiar with cloud platforms like Google Cloud (Vertex AI), AWS (SageMaker), or Azure (Azure AI) for training and deploying models.\n*   **Jupyter Notebooks/Google Colab:** Use these interactive environments for experimenting and prototyping.  Google Colab provides free access to GPUs.\n\n**5. Explore Available Resources:**\n\n*   **Online Courses:**\n    *   Coursera, edX, Udacity, fast.ai offer excellent courses on ML, DL, and NLP.\n    *   Specialized courses on GenAI are emerging rapidly.\n*   **Tutorials and Blogs:**\n    *   Towards Data Science (Medium)\n    *   Machine Learning Mastery\n    *   Papers with Code\n    *   Hugging Face blog\n*   **Research Papers:**\n    *   Keep up with the latest advancements by reading research papers on arXiv or Google Scholar. Don't try to understand everything at once; focus on the high-level concepts.\n*   **Communities:**\n    *   Join online communities like Reddit (r/MachineLearning, r/artificialintelligence), Discord servers, or forums.  Ask questions and learn from others.\n\n**6. Understand the Ethical Considerations:**\n\n*   **Bias:** GenAI models can perpetuate and amplify existing biases in the data they are trained on.\n*   **Misinformation:** GenAI can be used to create realistic fake content (deepfakes).\n*   **Copyright and Ownership:** Issues surrounding data used to train models and the output they generate.\n*   **Responsible Development:**  Think about the potential impact of your work and strive to create ethical and responsible GenAI applications.\n\n**7. Practice Regularly and Stay Curious:**\n\n*   **Consistent Effort:**  The key to success is consistent practice and learning.\n*   **Experiment:** Don't be afraid to try new things and experiment with different models and techniques.\n*   **Stay Updated:** GenAI is a fast-paced field. Keep up with the latest developments by reading research papers, following blogs, and attending conferences or webinars.\n*   **Document Your Learning:** Keep a journal or blog to document your learning process, projects, and insights. This will help you track your progress and solidify your understanding.\n\n**A Concrete First Project Idea:**\n\nA great first project is to fine-tune a pre-trained text generation model (like GPT-2) on a specific dataset. For example:\n\n1.  **Choose a Dataset:**  Find a dataset of text that interests you (e.g., song lyrics, recipes, short stories). Kaggle is a good resource.\n2.  **Load the Model:** Use the Hugging Face Transformers library to load a pre-trained GPT-2 model.\n3.  **Fine-Tune:** Fine-tune the model on your dataset.\n4.  **Generate Text:** Use the fine-tuned model to generate new text in the style of your dataset.\n\n**Key Takeaways for You, Parshva:**\n\n*   **Start small and build gradually.**\n*   **Focus on understanding the fundamentals.**\n*   **Don't be afraid to experiment and make mistakes.**\n*   **Join a community and learn from others.**\n*   **Be mindful of the ethical implications of GenAI.**\n*   **Most importantly: Have fun!**\n\nGood luck on your GenAI journey!  Let me know if you have any more specific questions as you progress.  I'm here to help.\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHf8fArfT0vd",
        "outputId": "328549a4-3e86-4f01-a288-c8feb3cf9e10"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n",
            "models/gemini-2.5-flash-live-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == 'models/gemini-2.5-flash':\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5JQSqdeT6kf",
        "outputId": "f116725f-f4c6-4bf9-c95b-308a2edf007a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Stable version of Gemini 2.5 Flash, our mid-size multimodal '\n",
            "                'model that supports up to 1 million tokens, released in June '\n",
            "                'of 2025.',\n",
            " 'display_name': 'Gemini 2.5 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.5-flash',\n",
            " 'output_token_limit': 65536,\n",
            " 'supported_actions': ['generateContent',\n",
            "                       'countTokens',\n",
            "                       'createCachedContent',\n",
            "                       'batchGenerateContent'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '001'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Temperature**\n",
        "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
        "\n",
        "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
      ],
      "metadata": {
        "id": "181T6C1uUrNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
        "# high_temp_config = types.GenerateContentConfig(temperature=0.0) #This is Low temperature. Try uncommenting above one and see change in output\n",
        "\n",
        "\n",
        "\n",
        "# for _ in range(5):\n",
        "#   response = client.models.generate_content(\n",
        "#       model='gemini-2.5-flash',\n",
        "#       config=high_temp_config,\n",
        "#       contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "#   if response.text:\n",
        "#     print(response.text, '-' * 25)"
      ],
      "metadata": {
        "id": "h3t2uUf8T_LZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Top-P**\n",
        "Like temperature, the top-P parameter is also used to control the diversity of the model's output.\n",
        "\n",
        "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
        "\n",
        "You may also see top-K referenced in LLM literature. Top-K is not configurable in the Gemini 2.0 series of models, but can be changed in older models. Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
        "\n",
        "Run this example a number of times, change the settings and observe the change in output."
      ],
      "metadata": {
        "id": "o9Wj5K_QVIBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_config = types.GenerateContentConfig(\n",
        "#     # These are the default values for gemini-2.0-flash.\n",
        "#     temperature=1.0,\n",
        "#     top_p=0.95,\n",
        "# )\n",
        "\n",
        "# story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=model_config,\n",
        "#     contents=story_prompt)\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "rU_Lo9neVTBL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompting**"
      ],
      "metadata": {
        "id": "0vqYrp65XhDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Zero Shot**\n",
        "\n",
        "Zero-shot prompts are prompts that describe the request for the model directly.\n",
        "\n",
        "**EXAMPLE**- Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment:"
      ],
      "metadata": {
        "id": "u0ExNwrcXmOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enum mode**\n",
        "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards."
      ],
      "metadata": {
        "id": "sZc118JWYCAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM**, short for Enumeration, is basically a fancy way of saying “a list of named options you can choose from.” It’s not some secret AI spell — it’s just a data structure used in programming (and yes, also in GenAI frameworks) to define a fixed set of values that something can take.\n",
        "\n",
        "Think of it like giving names to a few specific choices instead of letting someone type random junk. It’s like saying:\n",
        "\n",
        "“You can pick from {Cat, Dog, Hamster}, but not ‘DragonWithWiFi’.”"
      ],
      "metadata": {
        "id": "Bfz7sc61adDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you’re designing a GenAI pipeline where the model can only act in certain modes:\n",
        "\n",
        "    class AgentAction(Enum):\n",
        "\n",
        "      SUMMARIZE = \"summarize\"\n",
        "\n",
        "      TRANSLATE = \"translate\"\n",
        "\n",
        "      CODE = \"code\"\n",
        "\n",
        "\n",
        "So, if the model gets “SUMMARIZE”, it knows exactly what operation to perform. Keeps things neat, predictable, and stops the AI from hallucinating another mode called “make memes”."
      ],
      "metadata": {
        "id": "0qKInIFzarv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import enum\n",
        "\n",
        "# class Sentiment(enum.Enum):\n",
        "#     POSITIVE = \"positive\"\n",
        "#     NEUTRAL = \"neutral\"\n",
        "#     NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         response_mime_type=\"text/x.enum\",\n",
        "#         response_schema=Sentiment\n",
        "#     ),\n",
        "#     contents=zero_shot_prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "### OUTPUT would be \"positive\"\n",
        "\n",
        "\n",
        "# enum_response = response.parsed\n",
        "# print(enum_response)\n",
        "# print(type(enum_response))"
      ],
      "metadata": {
        "id": "g6m1c-YZXkAH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM** = a controlled vocabulary for your GenAI system.\n",
        "It ensures structure and sanity in a world full of chaotic data and even more chaotic humans.\n",
        "\n",
        "As Seneca said, “Order is what keeps the universe from sliding into chaos.”"
      ],
      "metadata": {
        "id": "xy9VlGWia6db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One-shot and few-shot**\n",
        "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt."
      ],
      "metadata": {
        "id": "Qpnxrr_sdE8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "# customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         temperature=0.1,\n",
        "#         top_p=1,\n",
        "#         max_output_tokens=250,\n",
        "#     ),\n",
        "#     contents=[few_shot_prompt, customer_order])\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "727sAItvYMAI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chain of Thought (CoT)**\n",
        "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
        "\n",
        "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
        "\n",
        "Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try."
      ],
      "metadata": {
        "id": "VQX-v-yljc0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "# am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "# Output -\n",
        "# 52"
      ],
      "metadata": {
        "id": "h8rsJjxZjf6N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "# I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# Markdown(response.text)\n",
        "\n",
        "# Output-\n",
        "# Here's how to solve this:\n",
        "\n",
        "# Find the age difference: When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
        "\n",
        "# Calculate the age difference: The age difference between you and your partner is 12 - 4 = 8 years.\n",
        "\n",
        "# Determine partner's current age: Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
        "\n",
        "# Answer: Your partner is currently 28 years old."
      ],
      "metadata": {
        "id": "wkUWbxxrjmKC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ReAct: Reason and act**\n",
        "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper."
      ],
      "metadata": {
        "id": "hQfRkvf4kuln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of an AI model just thinking (reasoning) or just doing (acting), ReAct makes it do both — in turns.\n",
        "\n",
        "The model reasons about the problem step-by-step, takes an action (like calling a tool, searching, or retrieving info), observes the result, and then continues reasoning.\n",
        "Think of it like a detective alternating between thinking out loud and doing stuff until the mystery’s solved."
      ],
      "metadata": {
        "id": "Hi5urWTwpWnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Step-by-step*\n",
        "\n",
        "Reasoning → the model thinks: “Hmm, what do I need to solve this?”\n",
        "\n",
        "Acting → it takes an action: “Let me look that up in the database.”\n",
        "\n",
        "Observation → it gets the result: “Okay, here’s what I found.”\n",
        "\n",
        "Repeat → it continues reasoning with the new info until it reaches the answer.\n",
        "\n",
        "So instead of blindly guessing, the model becomes something like a thoughtful agent that mixes logic with action — like a data scientist who actually tests their hypothesis instead of just tweeting it."
      ],
      "metadata": {
        "id": "wCXMNdF4pu1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Question - “What’s the current temperature in New York?”\n",
        "\n",
        "    The AI’s internal process might look like this:\n",
        "\n",
        "      Thought: I need real-time weather data.\n",
        "\n",
        "      Action: call_weather_api(\"New York\")\n",
        "\n",
        "      Observation: The API returns 12°C.\n",
        "\n",
        "      Thought: The temperature in New York is 12°C.\n",
        "\n",
        "      Final Answer: It’s 12°C in New York."
      ],
      "metadata": {
        "id": "8Z8iiLlcp_gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_instructions = \"\"\"\n",
        "# Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "# Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        "#  (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "#      will return some similar entities to search and you can try to search the information from those topics.\n",
        "#  (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "#      so keep your searches short.\n",
        "#  (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "# \"\"\"\n",
        "\n",
        "# example1 = \"\"\"Question\n",
        "# Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "# Thought 1\n",
        "# The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "# Action 1\n",
        "# <search>Milhouse</search>\n",
        "# Observation 1\n",
        "# Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "# Thought 2\n",
        "# The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "# Action 2\n",
        "# <lookup>named after</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "# Thought 3\n",
        "# Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "# Action 3\n",
        "# <finish>Richard Nixon</finish>\n",
        "# \"\"\"\n",
        "\n",
        "# example2 = \"\"\"Question\n",
        "# What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "# Thought 1\n",
        "# I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "# Action 1\n",
        "# <search>Colorado orogeny</search>\n",
        "\n",
        "# Observation 1\n",
        "# The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "# Thought 2\n",
        "# It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "# Action 2\n",
        "# <lookup>eastern sector</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "# Thought 3\n",
        "# The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "# Action 3\n",
        "# <search>High Plains</search>\n",
        "\n",
        "# Observation 3\n",
        "# High Plains refers to one of two distinct land regions\n",
        "\n",
        "# Thought 4\n",
        "# I need to instead search High Plains (United States).\n",
        "\n",
        "# Action 4\n",
        "# <search>High Plains (United States)</search>\n",
        "\n",
        "# Observation 4\n",
        "# The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "# Thought 5\n",
        "# High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "# Action 5\n",
        "# <finish>1,800 to 7,000 ft</finish>\n",
        "# \"\"\"\n",
        "# # Take a look through https://github.com/ysymyth/ReAct/\n"
      ],
      "metadata": {
        "id": "eucmY6UHkxpv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"\"\"Question\n",
        "# Who was the youngest author listed on the transformers NLP paper?\n",
        "# \"\"\"\n",
        "\n",
        "# # You will perform the Action; so generate up to, but not including, the Observation.\n",
        "# react_config = types.GenerateContentConfig(\n",
        "#     stop_sequences=[\"\\nObservation\"],\n",
        "#     system_instruction=model_instructions + example1 + example2,\n",
        "# )\n",
        "\n",
        "# # Create a chat that has the model instructions and examples pre-seeded.\n",
        "# react_chat = client.chats.create(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     config=react_config,\n",
        "# )\n",
        "\n",
        "# resp = react_chat.send_message(question)\n",
        "# print(resp.text)"
      ],
      "metadata": {
        "id": "vmjC5coRlMiP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Thinking mode**\n",
        "The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n",
        "\n",
        "Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated."
      ],
      "metadata": {
        "id": "MSHrgednQP9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "# from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "# response = client.models.generate_content_stream(\n",
        "#     model='gemini-2.0-flash-thinking-exp',\n",
        "#     contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        "# )\n",
        "\n",
        "# buf = io.StringIO()\n",
        "# for chunk in response:\n",
        "#     buf.write(chunk.text)\n",
        "#     # Display the response as it is streamed\n",
        "#     print(chunk.text, end='')\n",
        "\n",
        "# # And then render the finished response as formatted markdown.\n",
        "# clear_output()\n",
        "# Markdown(buf.getvalue())"
      ],
      "metadata": {
        "id": "OF8KliWnQSyE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Q&A with RAG using Chroma\n"
      ],
      "metadata": {
        "id": "0nmEocRvRbPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two big limitations of LLMs are 1) that they only \"know\" the information that they were trained on, and 2) that they have limited input context windows. A way to address both of these limitations is to use a technique called Retrieval Augmented Generation, or RAG. A RAG system has three stages:\n",
        "\n",
        "    Indexing\n",
        "    Retrieval\n",
        "    Generation"
      ],
      "metadata": {
        "id": "a2v-GQwBRgh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Indexing → Break documents into chunks, turn them into embeddings (numerical meaning), and store them in a vector database.\n",
        "\n",
        "2. Retrieval → When asked a question, find the most relevant chunks from that database using semantic similarity.\n",
        "\n",
        "3. Generation → Feed those chunks + the question to the LLM so it can craft a grounded, natural-language answer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  - Documents → [Indexing] → Vector DB\n",
        "\n",
        "  - User Query → [Retrieval] → Top Relevant Chunks\n",
        "\n",
        "  - Query + Chunks → [Generation] → Final Answer"
      ],
      "metadata": {
        "id": "CJjWD7WUTNB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG, short for Retrieval-Augmented Generation, is just a fancy combo move that helps AI answer questions based on real documents"
      ],
      "metadata": {
        "id": "YpwpNRTASoNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal LLMs are good at language but terrible at memory.\n",
        "You ask about some obscure company policy PDF — I can’t know it, because it’s not in my training data.\n",
        "\n",
        "So, RAG fixes that by giving the model access to your specific documents.\n",
        "\n",
        "It works like this:\n",
        "\n",
        "1. Store your documents somewhere searchable (usually as chunks of text in a “vector database” — basically a big mathy filing cabinet).\n",
        "\n",
        "2. When you ask a question, the system:\n",
        "\n",
        "    - Searches for the most relevant parts of those docs.\n",
        "\n",
        "    - Pulls out the top matches.\n",
        "\n",
        "3. Then it feeds both your question and those retrieved snippets into the language model.\n",
        "\n",
        "4. The model uses that context to generate a smart, grounded answer — not a wild guess.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UN6QDvzoSQ9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "    if \"embedContent\" in m.supported_actions:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfR8i125Rdt0",
        "outputId": "4acc1915-0366-4629-b7c1-6b1d364a52a6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Data"
      ],
      "metadata": {
        "id": "grN5EbpKVizu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\"\n",
        "DOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.'\n",
        "DOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"\n",
        "\n",
        "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
      ],
      "metadata": {
        "id": "D9Ri1rUdVkyi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CAuwwEceWG-p",
        "outputId": "0888be19-790b-46e0-e36a-907ed13f6bc9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.28.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=31b24f511f045c531dac4e9e09e062a2907da3e502987f4c837c4f929f42b2e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.3.4 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              },
              "id": "22a6c879e3954da39bc430eae07c9695"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "# Define a helper to retry when per-minute quota is reached.\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = \"retrieval_document\"\n",
        "        else:\n",
        "            embedding_task = \"retrieval_query\"\n",
        "\n",
        "        response = client.models.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            contents=input,\n",
        "            config=types.EmbedContentConfig(\n",
        "                task_type=embedding_task,\n",
        "            ),\n",
        "        )\n",
        "        return [e.values for e in response.embeddings]"
      ],
      "metadata": {
        "id": "yCOQ3NlfVnXp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "DB_NAME = \"googlecardb\"\n",
        "\n",
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
        "\n",
        "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAZsrX2kVwlU",
        "outputId": "408a5b48-d241-4469-f00c-8f9801ae4851"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4060984393.py:5: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  embed_fn = GeminiEmbeddingFunction()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9tPA-l2XFXM",
        "outputId": "5751cbe2-f01b-42c2-abc9-65b5e1e35ae2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to query mode when generating embeddings.\n",
        "embed_fn.document_mode = False\n",
        "\n",
        "# Search the Chroma DB using the specified query.\n",
        "query = \"How do you use the touchscreen to play music?\"\n",
        "\n",
        "result = db.query(query_texts=[query], n_results=1)\n",
        "[all_passages] = result[\"documents\"]\n",
        "\n",
        "Markdown(all_passages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "PCYhVsrxXKjY",
        "outputId": "65872512-1325-421d-ef3a-2893b2a560b8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs."
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_passages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI_KY52ba5Ud",
        "outputId": "dde03863-a3d8-45fa-b8eb-57fdad8ee501"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Augmented generation: Answer the question**\n",
        "\n",
        "Now that we’ve found a relevant passage from our document set during the retrieval step, we can move on to assembling a generation prompt and use the Gemini API to produce the final answer.\n",
        "\n",
        "In this example, we only retrieved a single passage. But in real-world scenarios — especially when dealing with a large collection of data — we’d typically retrieve multiple passages. That way, the Gemini model can decide which pieces of text are actually useful for answering the question.\n",
        "\n",
        "It’s perfectly fine if a few of those retrieved passages aren’t directly relevant; the model’s generation process is designed to filter out the noise and focus on what truly matters."
      ],
      "metadata": {
        "id": "4RUBd5M_aAn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_oneline = query.replace(\"\\n\", \" \")\n",
        "\n",
        "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
        "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "QUESTION: {query_oneline}\n",
        "\"\"\"\n",
        "\n",
        "# Add the retrieved documents to the prompt.\n",
        "for passage in all_passages:\n",
        "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
        "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ya2lOdKXhgB",
        "outputId": "3729f5bc-d11d-4719-ba86-f6a4d502a2dd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
            "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
            "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
            "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
            "\n",
            "QUESTION: How do you use the touchscreen to play music?\n",
            "PASSAGE: Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(answer.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "ooOuI5F6atCi",
        "outputId": "06f75975-d7b3-4d73-c1d1-4df94118c5c5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "It's super easy to play music in your Googlecar using the large touchscreen display! Since the touchscreen gives you access to all sorts of cool features, including entertainment, all you need to do to get your favorite tunes going is simply touch the \"Music\" icon on the screen."
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i7FjxkNG29VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hx9TfDg7JczQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}