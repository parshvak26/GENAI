{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK/GIW7To7eQa2zUnamH21",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parshvak26/GENAI/blob/main/Complete_GenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0E1pICdJNQM8"
      },
      "outputs": [],
      "source": [
        "# !pip install -U google-genai>=1.37.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "e48hX8yFO-lM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)\n"
      ],
      "metadata": {
        "id": "xc5s28y-PDYx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyASAH0l-sxogbn5lSZIQ07ei9YaM0MJAx8\"\n"
      ],
      "metadata": {
        "id": "w4MeSpouPKAX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain AI to me like I'm a kid.\"\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpTAA3w2PnZJ",
        "outputId": "70ad6e41-ab5d-4fa5-9a03-ce0e4110e4a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine you have a super-duper smart computer helper! That's kind of what AI is.\n",
            "\n",
            "**AI stands for Artificial Intelligence.**\n",
            "\n",
            "It's like giving a computer a \"brain\" that can learn and think, almost like you do! But it's not a real brain like yours with squishy bits, it's a very clever program.\n",
            "\n",
            "Here's what that smart computer helper can do:\n",
            "\n",
            "1.  **It can learn things:** Just like you learn to ride a bike or read a book, AI can learn by looking at lots and lots of information. If you show it a thousand pictures of cats, it learns what a cat looks like!\n",
            "\n",
            "2.  **It can figure things out:** Once it learns, it can use that knowledge to solve problems or make decisions.\n",
            "\n",
            "3.  **It can help us in many ways:**\n",
            "\n",
            "    *   **Talking to a computer:** When you ask Alexa or Google Assistant a question, or talk to Siri on an iPhone, that's AI trying to understand what you're saying and find an answer.\n",
            "    *   **Playing games:** The other players in your video game that aren't real people? Often, that's AI deciding what they should do next.\n",
            "    *   **Suggesting things:** When Netflix or YouTube shows you movies or videos it thinks you'll like, that's AI guessing based on what you've watched before.\n",
            "    *   **Recognizing faces:** When a phone unlocks by seeing your face, or a camera can tell who is in a picture, that's AI at work.\n",
            "    *   **Cars that drive themselves:** Some super fancy cars can use AI to \"see\" the road and drive all by themselves!\n",
            "\n",
            "So, think of AI as making computers smarter so they can help us do cool things, play fun games, and even understand what we're saying! It's like giving computers a little bit of magic thinking power. Isn't that neat?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "rudKuPzMSWMF",
        "outputId": "f7b2d8be-4925-4ea8-88c6-83de038c635f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, imagine you have a super-duper smart computer helper! That's kind of what AI is.\n\n**AI stands for Artificial Intelligence.**\n\nIt's like giving a computer a \"brain\" that can learn and think, almost like you do! But it's not a real brain like yours with squishy bits, it's a very clever program.\n\nHere's what that smart computer helper can do:\n\n1.  **It can learn things:** Just like you learn to ride a bike or read a book, AI can learn by looking at lots and lots of information. If you show it a thousand pictures of cats, it learns what a cat looks like!\n\n2.  **It can figure things out:** Once it learns, it can use that knowledge to solve problems or make decisions.\n\n3.  **It can help us in many ways:**\n\n    *   **Talking to a computer:** When you ask Alexa or Google Assistant a question, or talk to Siri on an iPhone, that's AI trying to understand what you're saying and find an answer.\n    *   **Playing games:** The other players in your video game that aren't real people? Often, that's AI deciding what they should do next.\n    *   **Suggesting things:** When Netflix or YouTube shows you movies or videos it thinks you'll like, that's AI guessing based on what you've watched before.\n    *   **Recognizing faces:** When a phone unlocks by seeing your face, or a camera can tell who is in a picture, that's AI at work.\n    *   **Cars that drive themselves:** Some super fancy cars can use AI to \"see\" the road and drive all by themselves!\n\nSo, think of AI as making computers smarter so they can help us do cool things, play fun games, and even understand what we're saying! It's like giving computers a little bit of magic thinking power. Isn't that neat?"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
        "response = chat.send_message('Hello! My name is Parshva. I am just starting with GENAI. Any tips?')\n"
      ],
      "metadata": {
        "id": "l5o5QhTLScN4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g1sXvAS6TwsZ",
        "outputId": "d5f6d0cf-7403-40b7-da53-37bbfab43d11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi Parshva, welcome to the world of Generative AI (GENAI)! It's an exciting field with a lot to learn. Here's a breakdown of tips to help you get started and navigate your GENAI journey:\n\n**1. Foundational Knowledge & Understanding:**\n\n*   **Understand the Core Concepts:**\n    *   **What is Generative AI?**  Grasp the basic principle: training AI models to generate *new* data, similar to the data they were trained on. Think of it as learning a style and then creating something *in* that style.\n    *   **Key GENAI Models:**  Become familiar with the big names:\n        *   **Large Language Models (LLMs):**  GPT (Generative Pre-trained Transformer) family (GPT-3, GPT-4), Bard/Gemini, LLaMA, Claude.  These are text-based models.\n        *   **Diffusion Models:** DALL-E, Stable Diffusion, Midjourney. These generate images.\n        *   **Generative Adversarial Networks (GANs):**  A classic approach. Two networks compete, one generating and one discriminating.  Less common now than diffusion models for images, but still useful.\n    *   **Training Data:**  Understand that GENAI models are *only* as good as their training data.  Biases, inaccuracies, and limited datasets can all significantly impact performance and output.\n*   **Learn Basic Machine Learning (ML) Fundamentals (Optional but Recommended):**\n    *   While you don't need to be an ML expert *immediately*, understanding basic concepts like:\n        *   **Supervised vs. Unsupervised Learning:** This gives context to how some GENAI models are trained (often a blend).\n        *   **Loss Functions:**  A general idea of how models learn to minimize errors.\n        *   **Neural Networks:**  The foundation for many GENAI models. Understanding the basics of layers, activation functions, and backpropagation is helpful.\n\n**2. Hands-on Exploration and Experimentation:**\n\n*   **Start with Pre-trained Models:**\n    *   **Use APIs and Platforms:**  The easiest way to get started is to use pre-trained models through APIs (Application Programming Interfaces) or web platforms.\n    *   **Examples:**\n        *   **OpenAI API:**  (GPT models, DALL-E).  Offers a generous free tier.\n        *   **Google AI Platform/Vertex AI:** (PaLM, Imagen, Gemini).\n        *   **Hugging Face:**  A fantastic community and platform with tons of pre-trained models (often open source) and demos.  Explore their \"Spaces\" for interactive examples.\n        *   **Microsoft Azure AI:** Provides access to a range of AI services including OpenAI models.\n    *   **Play with Prompts:**  Experiment with different prompts to see how the models respond.  This is critical for learning prompt engineering.\n*   **Prompt Engineering:**\n    *   **Learn the Basics:**  Prompt engineering is the art of crafting effective prompts to get the desired output from a GENAI model.\n    *   **Key Techniques:**\n        *   **Be Clear and Specific:**  The more precise your prompt, the better.\n        *   **Use Keywords:**  Include relevant keywords to guide the model.\n        *   **Provide Context:**  Give the model background information.\n        *   **Specify the Format:**  Tell the model the desired output format (e.g., \"Write a poem in the style of Shakespeare\").\n        *   **Few-Shot Learning:**  Give the model a few examples of the desired output in the prompt.\n        *   **Chain-of-Thought Prompting:**  Encourage the model to break down a complex task into smaller steps.\n    *   **Iterate and Refine:**  Experiment with different prompts and analyze the results to improve your prompts.\n*   **Explore Different Applications:**\n    *   **Text Generation:**  Summarization, translation, content creation (articles, stories, scripts), chatbot development.\n    *   **Image Generation:**  Creating images from text descriptions, image editing, style transfer.\n    *   **Code Generation:**  Assisting with coding tasks, generating code snippets.\n    *   **Music Generation:** Creating melodies, harmonies, and even full musical compositions.\n\n**3. Learning Resources:**\n\n*   **Online Courses and Tutorials:**\n    *   **Coursera, edX, Udacity, Udemy:**  Search for courses on \"Generative AI,\" \"Large Language Models,\" \"Deep Learning,\" and \"Natural Language Processing.\"\n    *   **DeepLearning.AI:** Andrew Ng's platform has excellent courses.\n    *   **Fast.ai:**  A practical, code-first approach to deep learning.\n*   **Books:**\n    *   Keep an eye out for updated books as the field is rapidly evolving.\n    *   Consider books on deep learning and neural networks to build a foundation.\n*   **Research Papers:**\n    *   **arXiv:**  A repository of pre-prints of scientific papers.\n    *   **Conference Proceedings:**  NeurIPS, ICML, ICLR are major machine learning conferences.  Check their proceedings for cutting-edge research.\n*   **Blogs and Newsletters:**\n    *   **The Batch (Andrew Ng's newsletter):**  A great source of news and insights.\n    *   **Machine Learning Mastery:**  Offers practical tutorials and resources.\n    *   Follow leading researchers and companies in the field on social media.\n*   **Communities:**\n    *   **Hugging Face Hub:**  A great place to connect with other GENAI enthusiasts.\n    *   **Reddit:**  r/MachineLearning, r/artificialintelligence.\n    *   **Discord Servers:**  Many communities have active Discord servers.\n\n**4. Ethical Considerations:**\n\n*   **Bias:**  Understand that GENAI models can perpetuate and amplify biases present in their training data.\n*   **Misinformation:**  Be aware of the potential for GENAI models to generate false or misleading information.\n*   **Copyright and Intellectual Property:**  Consider the legal implications of using GENAI models for commercial purposes.\n*   **Privacy:**  Be mindful of the data you use to train and use GENAI models.\n*   **Responsible Use:**  Think about the potential impact of your work and use GENAI responsibly.\n\n**5. Key Takeaways and Tips for Parshva:**\n\n*   **Start Small:** Don't try to learn everything at once. Focus on one area (e.g., text generation with GPT-3) and build from there.\n*   **Code Examples:**  Look for code examples and try to modify them.  Hands-on coding is crucial.\n*   **Stay Curious:**  Read articles, watch videos, and experiment with different models.\n*   **Network:** Connect with other people who are interested in GENAI.  Attend meetups, join online communities, and ask questions.\n*   **Document Your Learning:**  Keep a notebook (physical or digital) to record what you're learning, what you've tried, and what works.\n*   **Don't Be Afraid to Experiment:**  GENAI is a rapidly evolving field, so don't be afraid to try new things and push the boundaries.\n*   **Focus on Practical Applications:** Think about how you can use GENAI to solve real-world problems.\n*   **Be Patient:** Learning GENAI takes time and effort. Don't get discouraged if you don't understand everything right away.\n\n**Specific Advice for You, Parshva:**\n\n1.  **Identify Your Interests:**  What excites you most about GENAI? Is it writing stories, creating art, or something else? Focus your initial learning on that area.\n2.  **Set a Goal:** What do you want to be able to *do* with GENAI in the next month or two?  For example:  \"I want to be able to create a short story using a LLM.\"  Having a clear goal helps you stay focused.\n3.  **First Project Idea:**  A great starter project: Use the OpenAI API to create a simple chatbot that answers questions about a topic you're passionate about.  This will teach you about APIs, prompt engineering, and basic programming.\n\nGood luck on your GENAI journey!  It's a field full of possibilities. Remember to have fun and keep learning!  Feel free to ask if you have more specific questions as you progress.\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHf8fArfT0vd",
        "outputId": "cd08e27a-abd6-478d-d3cf-0e4ba5d10c83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-preview\n",
            "models/veo-3.0-fast-generate-preview\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n",
            "models/gemini-2.5-flash-live-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == 'models/gemini-2.5-flash':\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5JQSqdeT6kf",
        "outputId": "dd7c8774-ee7f-4e7e-f5a5-359013e9d305"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Stable version of Gemini 2.5 Flash, our mid-size multimodal '\n",
            "                'model that supports up to 1 million tokens, released in June '\n",
            "                'of 2025.',\n",
            " 'display_name': 'Gemini 2.5 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.5-flash',\n",
            " 'output_token_limit': 65536,\n",
            " 'supported_actions': ['generateContent',\n",
            "                       'countTokens',\n",
            "                       'createCachedContent',\n",
            "                       'batchGenerateContent'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '001'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Temperature**\n",
        "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
        "\n",
        "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
      ],
      "metadata": {
        "id": "181T6C1uUrNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
        "# high_temp_config = types.GenerateContentConfig(temperature=0.0) #This is Low temperature. Try uncommenting above one and see change in output\n",
        "\n",
        "\n",
        "\n",
        "# for _ in range(5):\n",
        "#   response = client.models.generate_content(\n",
        "#       model='gemini-2.5-flash',\n",
        "#       config=high_temp_config,\n",
        "#       contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "#   if response.text:\n",
        "#     print(response.text, '-' * 25)"
      ],
      "metadata": {
        "id": "h3t2uUf8T_LZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Top-P**\n",
        "Like temperature, the top-P parameter is also used to control the diversity of the model's output.\n",
        "\n",
        "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
        "\n",
        "You may also see top-K referenced in LLM literature. Top-K is not configurable in the Gemini 2.0 series of models, but can be changed in older models. Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
        "\n",
        "Run this example a number of times, change the settings and observe the change in output."
      ],
      "metadata": {
        "id": "o9Wj5K_QVIBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_config = types.GenerateContentConfig(\n",
        "#     # These are the default values for gemini-2.0-flash.\n",
        "#     temperature=1.0,\n",
        "#     top_p=0.95,\n",
        "# )\n",
        "\n",
        "# story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=model_config,\n",
        "#     contents=story_prompt)\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "rU_Lo9neVTBL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompting**"
      ],
      "metadata": {
        "id": "0vqYrp65XhDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Zero Shot**\n",
        "\n",
        "Zero-shot prompts are prompts that describe the request for the model directly.\n",
        "\n",
        "**EXAMPLE**- Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment:"
      ],
      "metadata": {
        "id": "u0ExNwrcXmOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enum mode**\n",
        "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards."
      ],
      "metadata": {
        "id": "sZc118JWYCAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM**, short for Enumeration, is basically a fancy way of saying “a list of named options you can choose from.” It’s not some secret AI spell — it’s just a data structure used in programming (and yes, also in GenAI frameworks) to define a fixed set of values that something can take.\n",
        "\n",
        "Think of it like giving names to a few specific choices instead of letting someone type random junk. It’s like saying:\n",
        "\n",
        "“You can pick from {Cat, Dog, Hamster}, but not ‘DragonWithWiFi’.”"
      ],
      "metadata": {
        "id": "Bfz7sc61adDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you’re designing a GenAI pipeline where the model can only act in certain modes:\n",
        "\n",
        "    class AgentAction(Enum):\n",
        "\n",
        "      SUMMARIZE = \"summarize\"\n",
        "\n",
        "      TRANSLATE = \"translate\"\n",
        "\n",
        "      CODE = \"code\"\n",
        "\n",
        "\n",
        "So, if the model gets “SUMMARIZE”, it knows exactly what operation to perform. Keeps things neat, predictable, and stops the AI from hallucinating another mode called “make memes”."
      ],
      "metadata": {
        "id": "0qKInIFzarv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import enum\n",
        "\n",
        "# class Sentiment(enum.Enum):\n",
        "#     POSITIVE = \"positive\"\n",
        "#     NEUTRAL = \"neutral\"\n",
        "#     NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         response_mime_type=\"text/x.enum\",\n",
        "#         response_schema=Sentiment\n",
        "#     ),\n",
        "#     contents=zero_shot_prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "### OUTPUT would be \"positive\"\n",
        "\n",
        "\n",
        "# enum_response = response.parsed\n",
        "# print(enum_response)\n",
        "# print(type(enum_response))"
      ],
      "metadata": {
        "id": "g6m1c-YZXkAH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM** = a controlled vocabulary for your GenAI system.\n",
        "It ensures structure and sanity in a world full of chaotic data and even more chaotic humans.\n",
        "\n",
        "As Seneca said, “Order is what keeps the universe from sliding into chaos.”"
      ],
      "metadata": {
        "id": "xy9VlGWia6db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One-shot and few-shot**\n",
        "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt."
      ],
      "metadata": {
        "id": "Qpnxrr_sdE8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "# customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         temperature=0.1,\n",
        "#         top_p=1,\n",
        "#         max_output_tokens=250,\n",
        "#     ),\n",
        "#     contents=[few_shot_prompt, customer_order])\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "727sAItvYMAI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chain of Thought (CoT)**\n",
        "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
        "\n",
        "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
        "\n",
        "Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try."
      ],
      "metadata": {
        "id": "VQX-v-yljc0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "# am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "# Output -\n",
        "# 52"
      ],
      "metadata": {
        "id": "h8rsJjxZjf6N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "# I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# Markdown(response.text)\n",
        "\n",
        "# Output-\n",
        "# Here's how to solve this:\n",
        "\n",
        "# Find the age difference: When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
        "\n",
        "# Calculate the age difference: The age difference between you and your partner is 12 - 4 = 8 years.\n",
        "\n",
        "# Determine partner's current age: Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
        "\n",
        "# Answer: Your partner is currently 28 years old."
      ],
      "metadata": {
        "id": "wkUWbxxrjmKC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ReAct: Reason and act**\n",
        "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper."
      ],
      "metadata": {
        "id": "hQfRkvf4kuln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of an AI model just thinking (reasoning) or just doing (acting), ReAct makes it do both — in turns.\n",
        "\n",
        "The model reasons about the problem step-by-step, takes an action (like calling a tool, searching, or retrieving info), observes the result, and then continues reasoning.\n",
        "Think of it like a detective alternating between thinking out loud and doing stuff until the mystery’s solved."
      ],
      "metadata": {
        "id": "Hi5urWTwpWnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Step-by-step*\n",
        "\n",
        "Reasoning → the model thinks: “Hmm, what do I need to solve this?”\n",
        "\n",
        "Acting → it takes an action: “Let me look that up in the database.”\n",
        "\n",
        "Observation → it gets the result: “Okay, here’s what I found.”\n",
        "\n",
        "Repeat → it continues reasoning with the new info until it reaches the answer.\n",
        "\n",
        "So instead of blindly guessing, the model becomes something like a thoughtful agent that mixes logic with action — like a data scientist who actually tests their hypothesis instead of just tweeting it."
      ],
      "metadata": {
        "id": "wCXMNdF4pu1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Question - “What’s the current temperature in New York?”\n",
        "\n",
        "    The AI’s internal process might look like this:\n",
        "\n",
        "      Thought: I need real-time weather data.\n",
        "\n",
        "      Action: call_weather_api(\"New York\")\n",
        "\n",
        "      Observation: The API returns 12°C.\n",
        "\n",
        "      Thought: The temperature in New York is 12°C.\n",
        "\n",
        "      Final Answer: It’s 12°C in New York."
      ],
      "metadata": {
        "id": "8Z8iiLlcp_gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_instructions = \"\"\"\n",
        "# Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "# Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        "#  (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "#      will return some similar entities to search and you can try to search the information from those topics.\n",
        "#  (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "#      so keep your searches short.\n",
        "#  (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "# \"\"\"\n",
        "\n",
        "# example1 = \"\"\"Question\n",
        "# Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "# Thought 1\n",
        "# The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "# Action 1\n",
        "# <search>Milhouse</search>\n",
        "# Observation 1\n",
        "# Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "# Thought 2\n",
        "# The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "# Action 2\n",
        "# <lookup>named after</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "# Thought 3\n",
        "# Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "# Action 3\n",
        "# <finish>Richard Nixon</finish>\n",
        "# \"\"\"\n",
        "\n",
        "# example2 = \"\"\"Question\n",
        "# What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "# Thought 1\n",
        "# I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "# Action 1\n",
        "# <search>Colorado orogeny</search>\n",
        "\n",
        "# Observation 1\n",
        "# The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "# Thought 2\n",
        "# It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "# Action 2\n",
        "# <lookup>eastern sector</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "# Thought 3\n",
        "# The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "# Action 3\n",
        "# <search>High Plains</search>\n",
        "\n",
        "# Observation 3\n",
        "# High Plains refers to one of two distinct land regions\n",
        "\n",
        "# Thought 4\n",
        "# I need to instead search High Plains (United States).\n",
        "\n",
        "# Action 4\n",
        "# <search>High Plains (United States)</search>\n",
        "\n",
        "# Observation 4\n",
        "# The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "# Thought 5\n",
        "# High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "# Action 5\n",
        "# <finish>1,800 to 7,000 ft</finish>\n",
        "# \"\"\"\n",
        "# # Take a look through https://github.com/ysymyth/ReAct/\n"
      ],
      "metadata": {
        "id": "eucmY6UHkxpv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"\"\"Question\n",
        "# Who was the youngest author listed on the transformers NLP paper?\n",
        "# \"\"\"\n",
        "\n",
        "# # You will perform the Action; so generate up to, but not including, the Observation.\n",
        "# react_config = types.GenerateContentConfig(\n",
        "#     stop_sequences=[\"\\nObservation\"],\n",
        "#     system_instruction=model_instructions + example1 + example2,\n",
        "# )\n",
        "\n",
        "# # Create a chat that has the model instructions and examples pre-seeded.\n",
        "# react_chat = client.chats.create(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     config=react_config,\n",
        "# )\n",
        "\n",
        "# resp = react_chat.send_message(question)\n",
        "# print(resp.text)"
      ],
      "metadata": {
        "id": "vmjC5coRlMiP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Thinking mode**\n",
        "The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n",
        "\n",
        "Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated."
      ],
      "metadata": {
        "id": "MSHrgednQP9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "# from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "# response = client.models.generate_content_stream(\n",
        "#     model='gemini-2.0-flash-thinking-exp',\n",
        "#     contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        "# )\n",
        "\n",
        "# buf = io.StringIO()\n",
        "# for chunk in response:\n",
        "#     buf.write(chunk.text)\n",
        "#     # Display the response as it is streamed\n",
        "#     print(chunk.text, end='')\n",
        "\n",
        "# # And then render the finished response as formatted markdown.\n",
        "# clear_output()\n",
        "# Markdown(buf.getvalue())"
      ],
      "metadata": {
        "id": "OF8KliWnQSyE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Q&A with RAG using Chroma\n"
      ],
      "metadata": {
        "id": "0nmEocRvRbPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two big limitations of LLMs are 1) that they only \"know\" the information that they were trained on, and 2) that they have limited input context windows. A way to address both of these limitations is to use a technique called Retrieval Augmented Generation, or RAG. A RAG system has three stages:\n",
        "\n",
        "    Indexing\n",
        "    Retrieval\n",
        "    Generation"
      ],
      "metadata": {
        "id": "a2v-GQwBRgh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Indexing → Break documents into chunks, turn them into embeddings (numerical meaning), and store them in a vector database.\n",
        "\n",
        "2. Retrieval → When asked a question, find the most relevant chunks from that database using semantic similarity.\n",
        "\n",
        "3. Generation → Feed those chunks + the question to the LLM so it can craft a grounded, natural-language answer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  - Documents → [Indexing] → Vector DB\n",
        "\n",
        "  - User Query → [Retrieval] → Top Relevant Chunks\n",
        "\n",
        "  - Query + Chunks → [Generation] → Final Answer"
      ],
      "metadata": {
        "id": "CJjWD7WUTNB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG, short for Retrieval-Augmented Generation, is just a fancy combo move that helps AI answer questions based on real documents"
      ],
      "metadata": {
        "id": "YpwpNRTASoNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal LLMs are good at language but terrible at memory.\n",
        "You ask about some obscure company policy PDF — I can’t know it, because it’s not in my training data.\n",
        "\n",
        "So, RAG fixes that by giving the model access to your specific documents.\n",
        "\n",
        "It works like this:\n",
        "\n",
        "1. Store your documents somewhere searchable (usually as chunks of text in a “vector database” — basically a big mathy filing cabinet).\n",
        "\n",
        "2. When you ask a question, the system:\n",
        "\n",
        "    - Searches for the most relevant parts of those docs.\n",
        "\n",
        "    - Pulls out the top matches.\n",
        "\n",
        "3. Then it feeds both your question and those retrieved snippets into the language model.\n",
        "\n",
        "4. The model uses that context to generate a smart, grounded answer — not a wild guess.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UN6QDvzoSQ9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "    if \"embedContent\" in m.supported_actions:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfR8i125Rdt0",
        "outputId": "f457b1bf-b52f-4231-8e6e-2eb7efe35ec3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Data"
      ],
      "metadata": {
        "id": "grN5EbpKVizu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\"\n",
        "DOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.'\n",
        "DOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"\n",
        "\n",
        "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
      ],
      "metadata": {
        "id": "D9Ri1rUdVkyi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAuwwEceWG-p",
        "outputId": "6a31b0e9-8628-4334-99e0-b0e00aa88121"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.4)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.28.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "# Define a helper to retry when per-minute quota is reached.\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = \"retrieval_document\"\n",
        "        else:\n",
        "            embedding_task = \"retrieval_query\"\n",
        "\n",
        "        response = client.models.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            contents=input,\n",
        "            config=types.EmbedContentConfig(\n",
        "                task_type=embedding_task,\n",
        "            ),\n",
        "        )\n",
        "        return [e.values for e in response.embeddings]"
      ],
      "metadata": {
        "id": "yCOQ3NlfVnXp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "DB_NAME = \"googlecardb\"\n",
        "\n",
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
        "\n",
        "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAZsrX2kVwlU",
        "outputId": "fe1db1d9-1939-4411-b152-7d58dedecf64"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4060984393.py:5: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  embed_fn = GeminiEmbeddingFunction()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9tPA-l2XFXM",
        "outputId": "9cfa2002-1c59-4c17-a2d8-438168c9169c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to query mode when generating embeddings.\n",
        "embed_fn.document_mode = False\n",
        "\n",
        "# Search the Chroma DB using the specified query.\n",
        "query = \"How do you use the touchscreen to play music?\"\n",
        "\n",
        "result = db.query(query_texts=[query], n_results=1)\n",
        "[all_passages] = result[\"documents\"]\n",
        "\n",
        "Markdown(all_passages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "PCYhVsrxXKjY",
        "outputId": "e6f3588b-3d1e-4d76-ab68-6ba2d9749495"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs."
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_passages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI_KY52ba5Ud",
        "outputId": "871a2f5e-58f1-4128-a2a3-7c7a3f88d01e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Augmented generation: Answer the question**\n",
        "\n",
        "Now that we’ve found a relevant passage from our document set during the retrieval step, we can move on to assembling a generation prompt and use the Gemini API to produce the final answer.\n",
        "\n",
        "In this example, we only retrieved a single passage. But in real-world scenarios — especially when dealing with a large collection of data — we’d typically retrieve multiple passages. That way, the Gemini model can decide which pieces of text are actually useful for answering the question.\n",
        "\n",
        "It’s perfectly fine if a few of those retrieved passages aren’t directly relevant; the model’s generation process is designed to filter out the noise and focus on what truly matters."
      ],
      "metadata": {
        "id": "4RUBd5M_aAn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_oneline = query.replace(\"\\n\", \" \")\n",
        "\n",
        "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
        "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "QUESTION: {query_oneline}\n",
        "\"\"\"\n",
        "\n",
        "# Add the retrieved documents to the prompt.\n",
        "for passage in all_passages:\n",
        "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
        "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ya2lOdKXhgB",
        "outputId": "9bf9d871-f6cd-4f65-f8e3-dbad43c3d20c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
            "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \n",
            "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
            "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
            "\n",
            "QUESTION: How do you use the touchscreen to play music?\n",
            "PASSAGE: Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(answer.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "ooOuI5F6atCi",
        "outputId": "4a3aad51-ee6e-40bb-9d31-7663dfbca132"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "It's super easy to play your favorite tunes on your Googlecar! All you need to do is simply touch the \"Music\" icon on the large touchscreen display, which is where you can access all sorts of entertainment features."
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVmyheG1bhgY"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}