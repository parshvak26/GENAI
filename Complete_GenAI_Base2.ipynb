{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXWMOEfWlGuJzrhwOoC41Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a44aaa4cc87462b9499fa727ed91797": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c4f52671cc2e4097b2ff8d7f3105711c",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16 \u001b[0m [ \u001b[33m0:01:41\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m0 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">16/16 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:01:41</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span> , <span style=\"color: #800000; text-decoration-color: #800000\">0 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c4f52671cc2e4097b2ff8d7f3105711c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parshvak26/GENAI/blob/main/Complete_GenAI_Base2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E1pICdJNQM8"
      },
      "outputs": [],
      "source": [
        "# !pip install -U google-genai>=1.37.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "e48hX8yFO-lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)\n"
      ],
      "metadata": {
        "id": "xc5s28y-PDYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YourAPIKEY\"\n"
      ],
      "metadata": {
        "id": "w4MeSpouPKAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain AI to me like I'm a kid.\"\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpTAA3w2PnZJ",
        "outputId": "f3d14c7e-b101-4fcc-efa9-2589039213a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine a computer that can *think* and *learn* a little bit, kind of like your own brain!\n",
            "\n",
            "That's what AI is: making computers super smart helpers that can learn and figure things out.\n",
            "\n",
            "Here's how it works:\n",
            "\n",
            "1.  **Learning:** Just like when you learn what a dog looks like by seeing many different dogs, AI learns by being shown lots and lots of examples. If you want it to know what a cat is, you show it thousands of pictures of cats until it starts to understand what makes a cat a cat.\n",
            "2.  **Helping:** Once it learns, it can do helpful things!\n",
            "\n",
            "**Where do you see AI?**\n",
            "\n",
            "*   **Talking Helpers:** Like Siri or Alexa on your mom or dad's phone, who can answer questions or play music when you ask. That's AI!\n",
            "*   **Recommendation Robot:** When you watch a show on Netflix or YouTube, and it suggests another show you might like, that's AI guessing what you'd enjoy.\n",
            "*   **Games:** Some characters in video games are smart and know how to play against you, thanks to AI.\n",
            "*   **Smart Cameras:** When your camera can recognize faces or help you take a really good picture, AI is often helping it out.\n",
            "*   **Robots:** Some robots that help build things in factories, or even clean your house, have AI brains to tell them what to do.\n",
            "\n",
            "So, AI is all about making computers clever problem-solvers that can learn from information and help us with lots of different tasks! It's like having a super-smart computer friend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "rudKuPzMSWMF",
        "outputId": "4d257528-2325-4c24-e1f1-823cf383e50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Imagine a computer that can *think* and *learn* a little bit, kind of like your own brain!\n\nThat's what AI is: making computers super smart helpers that can learn and figure things out.\n\nHere's how it works:\n\n1.  **Learning:** Just like when you learn what a dog looks like by seeing many different dogs, AI learns by being shown lots and lots of examples. If you want it to know what a cat is, you show it thousands of pictures of cats until it starts to understand what makes a cat a cat.\n2.  **Helping:** Once it learns, it can do helpful things!\n\n**Where do you see AI?**\n\n*   **Talking Helpers:** Like Siri or Alexa on your mom or dad's phone, who can answer questions or play music when you ask. That's AI!\n*   **Recommendation Robot:** When you watch a show on Netflix or YouTube, and it suggests another show you might like, that's AI guessing what you'd enjoy.\n*   **Games:** Some characters in video games are smart and know how to play against you, thanks to AI.\n*   **Smart Cameras:** When your camera can recognize faces or help you take a really good picture, AI is often helping it out.\n*   **Robots:** Some robots that help build things in factories, or even clean your house, have AI brains to tell them what to do.\n\nSo, AI is all about making computers clever problem-solvers that can learn from information and help us with lots of different tasks! It's like having a super-smart computer friend."
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
        "response = chat.send_message('Hello! My name is Parshva. I am just starting with GENAI. Any tips?')\n"
      ],
      "metadata": {
        "id": "l5o5QhTLScN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g1sXvAS6TwsZ",
        "outputId": "1ba60211-7413-494c-b8f7-84d07e8c8354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi Parshva, welcome to the exciting world of Generative AI (GenAI)! It's a rapidly evolving field with tons of potential. Here are some tips to help you get started:\n\n**1. Understand the Fundamentals:**\n\n*   **What is GenAI?** Broadly, it's about using AI models to *generate* new content – text, images, audio, video, code, and more. This is in contrast to traditional AI, which focuses on prediction or classification.\n*   **Key Models/Architectures:**\n    *   **Large Language Models (LLMs):** (e.g., GPT-3, LaMDA, LLaMA, Gemini, Claude) These are the powerhouse behind text generation, chatbots, and more.  Focus on understanding the Transformer architecture (attention mechanism).\n    *   **Diffusion Models:** (e.g., Stable Diffusion, DALL-E 2, Midjourney)  These excel at generating images from text prompts (text-to-image) and image editing.\n    *   **Generative Adversarial Networks (GANs):** (Not as dominant as diffusion models for image generation, but important to know.) Involve two neural networks that compete to generate increasingly realistic outputs.\n    *   **Variational Autoencoders (VAEs):** Another generative model.\n\n*   **Core Concepts:**\n    *   **Training Data:** GenAI models are trained on massive datasets.  The quality and biases in the training data directly affect the model's outputs.\n    *   **Fine-tuning:** Adapting a pre-trained model to a specific task or dataset.  This is often more efficient than training from scratch.\n    *   **Prompt Engineering:** Crafting effective prompts (inputs) to get the desired output from a model. This is a crucial skill.\n    *   **Tokens:**  LLMs process text by breaking it down into tokens. Understanding token limits is important for efficient use.\n    *   **Sampling Strategies:** Techniques used during inference (generation) to control the randomness and creativity of the output. Examples include temperature sampling, top-k sampling, top-p sampling (nucleus sampling).\n    *   **Embeddings:** Numerical representations of words, phrases, or concepts that capture their semantic meaning.  Used for tasks like semantic search and similarity analysis.\n\n**2. Hands-On Experience is Key:**\n\n*   **Play with Existing Tools:** The best way to learn is to experiment.  Use tools like:\n    *   **ChatGPT:** (openai.com) Great for text generation, summarization, question answering, coding, and more.\n    *   **Google Bard (now Gemini):** (bard.google.com) Another powerful LLM.\n    *   **DALL-E 2:** (openai.com/dall-e-2) For generating images from text.\n    *   **Midjourney:** (midjourney.com) Another leading image generation platform (Discord-based).\n    *   **Stable Diffusion:** (stability.ai)  Open-source image generation; can be run locally or on cloud platforms.\n    *   **GitHub Copilot:** (github.com/features/copilot) An AI pair programmer.\n    *   **Browse AI Platforms:**  Explore Hugging Face, Replicate, and others that offer a wide variety of pre-trained models and APIs for different tasks.\n\n*   **Start with Simple Projects:**\n    *   **Text Generation:** Write a story, poem, or blog post using ChatGPT or Bard.\n    *   **Image Generation:** Create images of specific objects, scenes, or styles with DALL-E 2 or Stable Diffusion.\n    *   **Code Generation:** Use GitHub Copilot to help you write code in a language you're learning.\n    *   **Summarization:** Use LLMs to summarize articles or documents.\n    *   **Translation:** Use LLMs to translate text between languages.\n\n**3. Learn to Prompt Effectively:**\n\n*   **Be Specific and Clear:**  The more precise your prompt, the better the results.\n*   **Provide Context:** Give the model enough background information to understand what you want.\n*   **Use Examples:**  Show the model examples of the desired output format or style.\n*   **Iterate and Refine:**  Experiment with different prompts and adjust them based on the results you get.\n*   **Understand Prompt Engineering Techniques:**  Explore techniques like:\n    *   **Few-Shot Learning:** Providing a few examples in the prompt.\n    *   **Chain-of-Thought Prompting:** Asking the model to explain its reasoning step-by-step.\n    *   **Prompt Chaining:** Combining multiple prompts to achieve a more complex task.\n\n**4. Dive Deeper into the Technology (Gradually):**\n\n*   **Online Courses:**\n    *   **Coursera, edX, Udacity, fast.ai:** Look for courses on deep learning, natural language processing (NLP), computer vision, and generative models. Andrew Ng's courses on Coursera are often a good starting point.\n    *   **Specialized GenAI Courses:** Search for courses specifically focused on generative AI.\n*   **Books:**\n    *   \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (the \"deep learning bible\") - more theoretical.\n    *   \"Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\" by Aurélien Géron - a more practical approach.\n*   **Research Papers:**  Start reading research papers from Arxiv.org to stay up-to-date on the latest advancements.  Focus on papers related to the models you're interested in.\n*   **Blogs and Websites:**\n    *   **Hugging Face Blog:** Excellent resources and tutorials.\n    *   **OpenAI Blog:** Updates on their research and products.\n    *   **Distill.pub:** Visual explanations of machine learning concepts.\n    *   **Papers with Code:** A good resource for finding code implementations of research papers.\n*   **Communities:**\n    *   **Hugging Face Forums:** Ask questions and connect with other practitioners.\n    *   **Reddit (r/MachineLearning, r/artificialintelligence):** Discuss the latest news and trends.\n    *   **Discord Servers:** Many communities exist around specific models or tools (e.g., Stable Diffusion).\n*   **Programming Languages and Frameworks:**\n    *   **Python:** The dominant language for AI.\n    *   **TensorFlow and PyTorch:** Popular deep learning frameworks.  PyTorch is often favored for research, while TensorFlow is commonly used in production.\n    *   **Hugging Face Transformers:** A powerful library for working with pre-trained models.\n\n**5. Ethical Considerations:**\n\n*   **Bias:** GenAI models can perpetuate and amplify biases present in their training data.\n*   **Misinformation:**  It's easy to create fake news, deepfakes, and other forms of misinformation.\n*   **Copyright and Intellectual Property:** The use of copyrighted material in training data raises legal and ethical questions.\n*   **Job Displacement:**  GenAI could automate certain tasks and lead to job losses.\n*   **Responsible AI Principles:** Familiarize yourself with responsible AI principles and guidelines from organizations like OpenAI, Google, and the Partnership on AI.\n\n**6. Stay Updated:**\n\n*   The field of GenAI is changing rapidly.  Follow researchers, companies, and influencers on social media (Twitter, LinkedIn) to stay informed.\n*   Attend conferences and workshops to learn from experts and network with other practitioners.\n\n**Important Reminders:**\n\n*   **Start small:** Don't try to learn everything at once. Focus on one area or model at a time.\n*   **Be patient:** It takes time and effort to learn GenAI. Don't get discouraged if you don't see results immediately.\n*   **Experiment and have fun:**  The best way to learn is to experiment and try new things.  Enjoy the process!\n*   **Don't blindly trust the output:** Always critically evaluate the output of GenAI models. They can make mistakes, generate biased content, or hallucinate information.\n\nGood luck, Parshva!  It's an exciting journey, and with consistent effort and exploration, you'll be well on your way to becoming proficient in Generative AI.\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHf8fArfT0vd",
        "outputId": "ef2225b2-7249-451e-e37f-fef5062112de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n",
            "models/gemini-2.5-flash-live-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == 'models/gemini-2.5-flash':\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5JQSqdeT6kf",
        "outputId": "fab0f219-29b5-4028-da48-86142ba33264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Stable version of Gemini 2.5 Flash, our mid-size multimodal '\n",
            "                'model that supports up to 1 million tokens, released in June '\n",
            "                'of 2025.',\n",
            " 'display_name': 'Gemini 2.5 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.5-flash',\n",
            " 'output_token_limit': 65536,\n",
            " 'supported_actions': ['generateContent',\n",
            "                       'countTokens',\n",
            "                       'createCachedContent',\n",
            "                       'batchGenerateContent'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '001'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Temperature**\n",
        "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
        "\n",
        "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
      ],
      "metadata": {
        "id": "181T6C1uUrNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
        "# high_temp_config = types.GenerateContentConfig(temperature=0.0) #This is Low temperature. Try uncommenting above one and see change in output\n",
        "\n",
        "\n",
        "\n",
        "# for _ in range(5):\n",
        "#   response = client.models.generate_content(\n",
        "#       model='gemini-2.5-flash',\n",
        "#       config=high_temp_config,\n",
        "#       contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "#   if response.text:\n",
        "#     print(response.text, '-' * 25)"
      ],
      "metadata": {
        "id": "h3t2uUf8T_LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Top-P**\n",
        "Like temperature, the top-P parameter is also used to control the diversity of the model's output.\n",
        "\n",
        "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
        "\n",
        "You may also see top-K referenced in LLM literature. Top-K is not configurable in the Gemini 2.0 series of models, but can be changed in older models. Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
        "\n",
        "Run this example a number of times, change the settings and observe the change in output."
      ],
      "metadata": {
        "id": "o9Wj5K_QVIBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_config = types.GenerateContentConfig(\n",
        "#     # These are the default values for gemini-2.0-flash.\n",
        "#     temperature=1.0,\n",
        "#     top_p=0.95,\n",
        "# )\n",
        "\n",
        "# story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=model_config,\n",
        "#     contents=story_prompt)\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "rU_Lo9neVTBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompting**"
      ],
      "metadata": {
        "id": "0vqYrp65XhDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Zero Shot**\n",
        "\n",
        "Zero-shot prompts are prompts that describe the request for the model directly.\n",
        "\n",
        "**EXAMPLE**- Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment:"
      ],
      "metadata": {
        "id": "u0ExNwrcXmOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enum mode**\n",
        "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards."
      ],
      "metadata": {
        "id": "sZc118JWYCAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM**, short for Enumeration, is basically a fancy way of saying “a list of named options you can choose from.” It’s not some secret AI spell — it’s just a data structure used in programming (and yes, also in GenAI frameworks) to define a fixed set of values that something can take.\n",
        "\n",
        "Think of it like giving names to a few specific choices instead of letting someone type random junk. It’s like saying:\n",
        "\n",
        "“You can pick from {Cat, Dog, Hamster}, but not ‘DragonWithWiFi’.”"
      ],
      "metadata": {
        "id": "Bfz7sc61adDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you’re designing a GenAI pipeline where the model can only act in certain modes:\n",
        "\n",
        "    class AgentAction(Enum):\n",
        "\n",
        "      SUMMARIZE = \"summarize\"\n",
        "\n",
        "      TRANSLATE = \"translate\"\n",
        "\n",
        "      CODE = \"code\"\n",
        "\n",
        "\n",
        "So, if the model gets “SUMMARIZE”, it knows exactly what operation to perform. Keeps things neat, predictable, and stops the AI from hallucinating another mode called “make memes”."
      ],
      "metadata": {
        "id": "0qKInIFzarv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import enum\n",
        "\n",
        "# class Sentiment(enum.Enum):\n",
        "#     POSITIVE = \"positive\"\n",
        "#     NEUTRAL = \"neutral\"\n",
        "#     NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         response_mime_type=\"text/x.enum\",\n",
        "#         response_schema=Sentiment\n",
        "#     ),\n",
        "#     contents=zero_shot_prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "### OUTPUT would be \"positive\"\n",
        "\n",
        "\n",
        "# enum_response = response.parsed\n",
        "# print(enum_response)\n",
        "# print(type(enum_response))"
      ],
      "metadata": {
        "id": "g6m1c-YZXkAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM** = a controlled vocabulary for your GenAI system.\n",
        "It ensures structure and sanity in a world full of chaotic data and even more chaotic humans.\n",
        "\n",
        "As Seneca said, “Order is what keeps the universe from sliding into chaos.”"
      ],
      "metadata": {
        "id": "xy9VlGWia6db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One-shot and few-shot**\n",
        "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt."
      ],
      "metadata": {
        "id": "Qpnxrr_sdE8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "# customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         temperature=0.1,\n",
        "#         top_p=1,\n",
        "#         max_output_tokens=250,\n",
        "#     ),\n",
        "#     contents=[few_shot_prompt, customer_order])\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "727sAItvYMAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chain of Thought (CoT)**\n",
        "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
        "\n",
        "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
        "\n",
        "Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try."
      ],
      "metadata": {
        "id": "VQX-v-yljc0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "# am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "# Output -\n",
        "# 52"
      ],
      "metadata": {
        "id": "h8rsJjxZjf6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "# I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# Markdown(response.text)\n",
        "\n",
        "# Output-\n",
        "# Here's how to solve this:\n",
        "\n",
        "# Find the age difference: When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
        "\n",
        "# Calculate the age difference: The age difference between you and your partner is 12 - 4 = 8 years.\n",
        "\n",
        "# Determine partner's current age: Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
        "\n",
        "# Answer: Your partner is currently 28 years old."
      ],
      "metadata": {
        "id": "wkUWbxxrjmKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ReAct: Reason and act**\n",
        "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper."
      ],
      "metadata": {
        "id": "hQfRkvf4kuln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of an AI model just thinking (reasoning) or just doing (acting), ReAct makes it do both — in turns.\n",
        "\n",
        "The model reasons about the problem step-by-step, takes an action (like calling a tool, searching, or retrieving info), observes the result, and then continues reasoning.\n",
        "Think of it like a detective alternating between thinking out loud and doing stuff until the mystery’s solved."
      ],
      "metadata": {
        "id": "Hi5urWTwpWnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Step-by-step*\n",
        "\n",
        "Reasoning → the model thinks: “Hmm, what do I need to solve this?”\n",
        "\n",
        "Acting → it takes an action: “Let me look that up in the database.”\n",
        "\n",
        "Observation → it gets the result: “Okay, here’s what I found.”\n",
        "\n",
        "Repeat → it continues reasoning with the new info until it reaches the answer.\n",
        "\n",
        "So instead of blindly guessing, the model becomes something like a thoughtful agent that mixes logic with action — like a data scientist who actually tests their hypothesis instead of just tweeting it."
      ],
      "metadata": {
        "id": "wCXMNdF4pu1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Question - “What’s the current temperature in New York?”\n",
        "\n",
        "    The AI’s internal process might look like this:\n",
        "\n",
        "      Thought: I need real-time weather data.\n",
        "\n",
        "      Action: call_weather_api(\"New York\")\n",
        "\n",
        "      Observation: The API returns 12°C.\n",
        "\n",
        "      Thought: The temperature in New York is 12°C.\n",
        "\n",
        "      Final Answer: It’s 12°C in New York."
      ],
      "metadata": {
        "id": "8Z8iiLlcp_gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_instructions = \"\"\"\n",
        "# Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "# Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        "#  (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "#      will return some similar entities to search and you can try to search the information from those topics.\n",
        "#  (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "#      so keep your searches short.\n",
        "#  (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "# \"\"\"\n",
        "\n",
        "# example1 = \"\"\"Question\n",
        "# Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "# Thought 1\n",
        "# The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "# Action 1\n",
        "# <search>Milhouse</search>\n",
        "# Observation 1\n",
        "# Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "# Thought 2\n",
        "# The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "# Action 2\n",
        "# <lookup>named after</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "# Thought 3\n",
        "# Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "# Action 3\n",
        "# <finish>Richard Nixon</finish>\n",
        "# \"\"\"\n",
        "\n",
        "# example2 = \"\"\"Question\n",
        "# What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "# Thought 1\n",
        "# I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "# Action 1\n",
        "# <search>Colorado orogeny</search>\n",
        "\n",
        "# Observation 1\n",
        "# The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "# Thought 2\n",
        "# It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "# Action 2\n",
        "# <lookup>eastern sector</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "# Thought 3\n",
        "# The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "# Action 3\n",
        "# <search>High Plains</search>\n",
        "\n",
        "# Observation 3\n",
        "# High Plains refers to one of two distinct land regions\n",
        "\n",
        "# Thought 4\n",
        "# I need to instead search High Plains (United States).\n",
        "\n",
        "# Action 4\n",
        "# <search>High Plains (United States)</search>\n",
        "\n",
        "# Observation 4\n",
        "# The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "# Thought 5\n",
        "# High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "# Action 5\n",
        "# <finish>1,800 to 7,000 ft</finish>\n",
        "# \"\"\"\n",
        "# # Take a look through https://github.com/ysymyth/ReAct/\n"
      ],
      "metadata": {
        "id": "eucmY6UHkxpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"\"\"Question\n",
        "# Who was the youngest author listed on the transformers NLP paper?\n",
        "# \"\"\"\n",
        "\n",
        "# # You will perform the Action; so generate up to, but not including, the Observation.\n",
        "# react_config = types.GenerateContentConfig(\n",
        "#     stop_sequences=[\"\\nObservation\"],\n",
        "#     system_instruction=model_instructions + example1 + example2,\n",
        "# )\n",
        "\n",
        "# # Create a chat that has the model instructions and examples pre-seeded.\n",
        "# react_chat = client.chats.create(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     config=react_config,\n",
        "# )\n",
        "\n",
        "# resp = react_chat.send_message(question)\n",
        "# print(resp.text)"
      ],
      "metadata": {
        "id": "vmjC5coRlMiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Thinking mode**\n",
        "The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n",
        "\n",
        "Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated."
      ],
      "metadata": {
        "id": "MSHrgednQP9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "# from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "# response = client.models.generate_content_stream(\n",
        "#     model='gemini-2.0-flash-thinking-exp',\n",
        "#     contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        "# )\n",
        "\n",
        "# buf = io.StringIO()\n",
        "# for chunk in response:\n",
        "#     buf.write(chunk.text)\n",
        "#     # Display the response as it is streamed\n",
        "#     print(chunk.text, end='')\n",
        "\n",
        "# # And then render the finished response as formatted markdown.\n",
        "# clear_output()\n",
        "# Markdown(buf.getvalue())"
      ],
      "metadata": {
        "id": "OF8KliWnQSyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Q&A with RAG using Chroma\n"
      ],
      "metadata": {
        "id": "0nmEocRvRbPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two big limitations of LLMs are 1) that they only \"know\" the information that they were trained on, and 2) that they have limited input context windows. A way to address both of these limitations is to use a technique called Retrieval Augmented Generation, or RAG. A RAG system has three stages:\n",
        "\n",
        "    Indexing\n",
        "    Retrieval\n",
        "    Generation"
      ],
      "metadata": {
        "id": "a2v-GQwBRgh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Indexing → Break documents into chunks, turn them into embeddings (numerical meaning), and store them in a vector database.\n",
        "\n",
        "2. Retrieval → When asked a question, find the most relevant chunks from that database using semantic similarity.\n",
        "\n",
        "3. Generation → Feed those chunks + the question to the LLM so it can craft a grounded, natural-language answer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  - Documents → [Indexing] → Vector DB\n",
        "\n",
        "  - User Query → [Retrieval] → Top Relevant Chunks\n",
        "\n",
        "  - Query + Chunks → [Generation] → Final Answer"
      ],
      "metadata": {
        "id": "CJjWD7WUTNB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG, short for Retrieval-Augmented Generation, is just a fancy combo move that helps AI answer questions based on real documents"
      ],
      "metadata": {
        "id": "YpwpNRTASoNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal LLMs are good at language but terrible at memory.\n",
        "You ask about some obscure company policy PDF — I can’t know it, because it’s not in my training data.\n",
        "\n",
        "So, RAG fixes that by giving the model access to your specific documents.\n",
        "\n",
        "It works like this:\n",
        "\n",
        "1. Store your documents somewhere searchable (usually as chunks of text in a “vector database” — basically a big mathy filing cabinet).\n",
        "\n",
        "2. When you ask a question, the system:\n",
        "\n",
        "    - Searches for the most relevant parts of those docs.\n",
        "\n",
        "    - Pulls out the top matches.\n",
        "\n",
        "3. Then it feeds both your question and those retrieved snippets into the language model.\n",
        "\n",
        "4. The model uses that context to generate a smart, grounded answer — not a wild guess.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UN6QDvzoSQ9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "    if \"embedContent\" in m.supported_actions:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfR8i125Rdt0",
        "outputId": "24c62c0e-b4ad-47af-fb4c-3637843b21f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Data"
      ],
      "metadata": {
        "id": "grN5EbpKVizu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\"\n",
        "DOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.'\n",
        "DOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"\n",
        "\n",
        "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
      ],
      "metadata": {
        "id": "D9Ri1rUdVkyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install chromadb\n"
      ],
      "metadata": {
        "id": "CAuwwEceWG-p",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "# Define a helper to retry when per-minute quota is reached.\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = \"retrieval_document\"\n",
        "        else:\n",
        "            embedding_task = \"retrieval_query\"\n",
        "\n",
        "        response = client.models.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            contents=input,\n",
        "            config=types.EmbedContentConfig(\n",
        "                task_type=embedding_task,\n",
        "            ),\n",
        "        )\n",
        "        return [e.values for e in response.embeddings]"
      ],
      "metadata": {
        "id": "yCOQ3NlfVnXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "DB_NAME = \"googlecardb\"\n",
        "\n",
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
        "\n",
        "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAZsrX2kVwlU",
        "outputId": "58b14956-8e20-4433-d042-bc5498aa1ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4060984393.py:5: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  embed_fn = GeminiEmbeddingFunction()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9tPA-l2XFXM",
        "outputId": "6fbdc3db-06f2-439b-e194-368e8dd01102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to query mode when generating embeddings.\n",
        "embed_fn.document_mode = False\n",
        "\n",
        "# Search the Chroma DB using the specified query.\n",
        "query = \"How do you use the touchscreen to play music?\"\n",
        "\n",
        "result = db.query(query_texts=[query], n_results=1)\n",
        "[all_passages] = result[\"documents\"]\n",
        "\n",
        "Markdown(all_passages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "PCYhVsrxXKjY",
        "outputId": "b244fc93-a314-487a-80dc-3aefff6c3640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs."
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_passages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI_KY52ba5Ud",
        "outputId": "c0499f48-2177-4012-a123-991004da730e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Augmented generation: Answer the question**\n",
        "\n",
        "Now that we’ve found a relevant passage from our document set during the retrieval step, we can move on to assembling a generation prompt and use the Gemini API to produce the final answer.\n",
        "\n",
        "In this example, we only retrieved a single passage. But in real-world scenarios — especially when dealing with a large collection of data — we’d typically retrieve multiple passages. That way, the Gemini model can decide which pieces of text are actually useful for answering the question.\n",
        "\n",
        "It’s perfectly fine if a few of those retrieved passages aren’t directly relevant; the model’s generation process is designed to filter out the noise and focus on what truly matters."
      ],
      "metadata": {
        "id": "4RUBd5M_aAn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_oneline = query.replace(\"\\n\", \" \")\n",
        "\n",
        "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
        "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "QUESTION: {query_oneline}\n",
        "\"\"\"\n",
        "\n",
        "# Add the retrieved documents to the prompt.\n",
        "for passage in all_passages:\n",
        "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
        "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ya2lOdKXhgB",
        "outputId": "4aefc9f6-7ef9-4c5e-d145-d19599db0c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
            "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
            "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
            "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
            "\n",
            "QUESTION: How do you use the touchscreen to play music?\n",
            "PASSAGE: Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(answer.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooOuI5F6atCi",
        "outputId": "b679c2ff-4cc2-4d61-9313-4308f603aee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Playing music in your Googlecar is super easy! All you need to do is look at the large touchscreen display in your car and simply touch the \"Music\" icon you see there, and you'll be able to enjoy your favorite songs."
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning Model"
      ],
      "metadata": {
        "id": "nRo6fpJ3lsI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning means taking a pretrained model (like Gemini, GPT, or Llama) and teaching it new tricks — not from scratch, but by giving it extra, specific examples so it learns your tone, domain, or task.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        " - The base model = a smart intern who knows everything in general.\n",
        "\n",
        " - Fine-tuning = teaching that intern how your company does things.\n",
        "\n",
        "You’re not retraining their whole brain — you’re just nudging their habits."
      ],
      "metadata": {
        "id": "wqw0A3xxl1YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Start with a pretrained model**\n",
        "\n",
        "    You don’t build from zero — you take an existing model like:\n",
        "\n",
        "    text-bison, gemini, gpt-3.5-turbo, etc.\n",
        "    These models already know grammar, logic, reasoning — basically the “universal stuff.”\n",
        "\n",
        "2. **Prepare your custom data**\n",
        "\n",
        "    You create a dataset that shows the model how you want it to behave.\n",
        "    Usually looks like this:\n",
        "\n",
        "        {\"input\": \"What is your refund policy?\", \"output\": \"Refunds are processed in 14 days.\"}\n",
        "  \n",
        "    OR\n",
        "  \n",
        "        {\"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Summarize this report.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Here’s a short summary...\"}\n",
        "        ]}\n",
        "    \n",
        "    You collect hundreds or thousands of such examples. The more clean and consistent, the better.\n",
        "\n",
        "3. **Train the model**\n",
        "\n",
        "    You feed that dataset into the fine-tuning API (e.g., Google’s Vertex AI, OpenAI’s fine-tuning endpoint, etc.).\n",
        "\n",
        "    Under the hood:\n",
        "\n",
        "    - The base weights are slightly adjusted to prefer your examples.\n",
        "\n",
        "    - The model’s general knowledge stays intact.\n",
        "\n",
        "    This step takes anywhere from minutes to hours depending on your data and compute.\n",
        "\n",
        "4. **Deploy your fine-tuned model**\n",
        "\n",
        "    You’ll get a new model ID, something like:\n",
        "\n",
        "        projects/your-id/locations/us-central1/models/fine-tuned-customer-support\n",
        "\n",
        "    Now you use that instead of the base model in your API calls.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "rmG99doNmZbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Use Cases**\n",
        "\n",
        " - Customer support: Model learns your company’s tone, product facts, policies.\n",
        "\n",
        " - Code generation: Teach it your team’s code style, naming conventions.\n",
        "\n",
        " - Legal / medical text: Make it speak your industry’s language.\n",
        "\n",
        " - Creative writing: Train it to write like you, not a Wikipedia article."
      ],
      "metadata": {
        "id": "_Qmi9ypRoo10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Fine-Tuning isn’t**\n",
        "\n",
        " - It’s not for teaching brand-new world knowledge.\n",
        "(You can’t fine-tune GPT on tomorrow’s news — that’s RAG’s job.)\n",
        "\n",
        " - It’s not for fixing hallucinations.\n",
        "It’s for improving style and consistency.\n",
        "\n",
        " - And it’s definitely not cheap — those GPUs are thirsty."
      ],
      "metadata": {
        "id": "nLbzn524o4cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset=\"train\")\n",
        "newsgroups_test = fetch_20newsgroups(subset=\"test\")\n",
        "\n",
        "# View list of class names for dataset\n",
        "newsgroups_train.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7FjxkNG29VN",
        "outputId": "3e3be600-72ef-461c-9993-31bcf0206437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsgroups_train.data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx9TfDg7JczQ",
        "outputId": "51f9bc2f-7335-43d3-8896-c4e8647959dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# newsgroups_train"
      ],
      "metadata": {
        "id": "zb9B5Wp0qeQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'data': [\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\","
      ],
      "metadata": {
        "id": "roFQv25ot_7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_newsgroup_row(data):\n",
        "    # Split headers from body manually\n",
        "    parts = data.split(\"\\n\\n\", 1)\n",
        "    headers = parts[0] if len(parts) > 0 else \"\"\n",
        "    body = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    # Extract the Subject field manually\n",
        "    subject = \"\"\n",
        "    for line in headers.split(\"\\n\"):\n",
        "        if line.lower().startswith(\"subject:\"):\n",
        "            subject = line[len(\"subject:\"):].strip()\n",
        "            break\n",
        "\n",
        "    # Build text\n",
        "    text = f\"{subject}\\n\\n{body}\"\n",
        "\n",
        "    # Strip email addresses\n",
        "    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n",
        "\n",
        "    # Truncate\n",
        "    return text[:40000]\n",
        "\n",
        "\n",
        "def preprocess_newsgroup_data(newsgroup_dataset):\n",
        "    df = pd.DataFrame({\n",
        "        \"Text\": newsgroup_dataset.data,\n",
        "        \"Label\": newsgroup_dataset.target\n",
        "    })\n",
        "\n",
        "    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n",
        "    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "tVHcHLQftjD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = preprocess_newsgroup_data(newsgroups_train)\n",
        "df_test = preprocess_newsgroup_data(newsgroups_test)\n",
        "\n",
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5Kxm8F1tmRj",
        "outputId": "393754e0-38bf-442d-8f84-90120844981f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  Label  \\\n",
              "0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n",
              "1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n",
              "2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n",
              "3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...      1   \n",
              "4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n",
              "\n",
              "              Class Name  \n",
              "0              rec.autos  \n",
              "1  comp.sys.mac.hardware  \n",
              "2  comp.sys.mac.hardware  \n",
              "3          comp.graphics  \n",
              "4              sci.space  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-269a8fc4-5ffa-4ff3-9366-8188329cc27d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "      <th>Class Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WHAT car is this!?\\n\\n I was wondering if anyo...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SI Clock Poll - Final Call\\n\\nA fair number of...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PB questions...\\n\\nwell folks, my mac plus fin...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...</td>\n",
              "      <td>1</td>\n",
              "      <td>comp.graphics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-269a8fc4-5ffa-4ff3-9366-8188329cc27d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-269a8fc4-5ffa-4ff3-9366-8188329cc27d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-269a8fc4-5ffa-4ff3-9366-8188329cc27d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cc97d77a-f115-499e-8c45-6b7cbbc65e47\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc97d77a-f115-499e-8c45-6b7cbbc65e47')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cc97d77a-f115-499e-8c45-6b7cbbc65e47 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 11314,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11308,\n        \"samples\": [\n          \"Re: History question\\n\\nIn article <>  (Paul Johnson) writes:\\n>\\n>I recall reading of a phonograph which used mechanical amplification.\\n>Compressed air was squirted out of a valve which was controlled by the\\n>pickup.  The result was noisy and distinctly lo-fi, but much louder\\n>than a conventional phonograph.  It tended to wear the disks out\\n>pretty quickly though.\\n\\nThis was the Pathe you are thinking of, although there were other imitators.\\nIt didn't wear the disks any more than conventional acoustic designs, but\\nit did have a high noise level due to the continual hiss of escaping air.\\nThere are a lot of them still operating, and they are pretty ingenious.\\n\\nThere was a pneumatic amplifier designed by Alexander Graham Bell, as well,\\nbut I don't know if it was ever constructed.\\n--scott\\n\",\n          \"Emulator pods\\n\\nA surplus-dealing buddy of mine came up with two emulator pods:\\n\\n\\tHP64220C (for HP 64100 development station). 8086 target\\n\\tprocessor. DIP head. Does not include board that plugs into\\n\\tthe 64100.\\n\\n\\tApplied Microsystems 80C186/188 pod, LCC head.\\n\\nIf you have an interest in either, let me know. They look to\\nbe in excellent condition. He doesn't know what to do with them, which\\nmay mean that they'll be cheap.\\n\\n-- \\n--------------------------------------------------------------------\\n       Dave Medin\\t\\t\\tPhone:\\t(205) 730-3169 (w)\\n    SSD--Networking\\t\\t\\t\\t(205) 837-1174 (h)\\n    Intergraph Corp.\\n       M/S GD3004 \\t\\tInternet: \\n  Huntsville, AL 35894\\t\\tUUCP:  ...uunet!ingr!b30!catbyte!dtmedin\\n\\n   ******* Everywhere You Look (at least around my office) *******\\n\\n * The opinions expressed here are mine (or those of my machine)\\n\",\n          \"Re: VGA 640x400 graphics mode\\n\\n writes in article <>:\\n> \\n> Greetings!\\n> \\n> Does anybody know if it is possible to set VGA graphics mode to 640x400\\n> instead of 640x480?  Any info is appreciated!\\n\\nSome VESA bios's support this mode (0x100).  And *any* VGA should be able to\\nsupport this (640x480 by 256 colors) since it only requires 256,000 bytes.\\nMy 8514/a VESA TSR supports this; it's the only VESA mode by card can support\\ndue to 8514/a restrictions. (A WD/Paradise)\\n\\n--\\nI am not responsible for anything I do or say -- I'm just an opinion.\\n             Robert J.C. Kyanko ()\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 0,\n        \"max\": 19,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          7,\n          17,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Class Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"rec.autos\",\n          \"talk.politics.mideast\",\n          \"rec.sport.baseball\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_data(df, num_samples, classes_to_keep):\n",
        "    # Sample rows, selecting num_samples of each Label.\n",
        "    df = (\n",
        "        df.groupby(\"Label\")[df.columns]\n",
        "        .apply(lambda x: x.sample(num_samples))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n",
        "    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "TRAIN_NUM_SAMPLES = 50\n",
        "TEST_NUM_SAMPLES = 10\n",
        "# Keep rec.* and sci.*\n",
        "CLASSES_TO_KEEP = \"^rec|^sci\"\n",
        "\n",
        "df_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\n",
        "df_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)\n"
      ],
      "metadata": {
        "id": "ZbCD0QQZytLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0\n",
        "sample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx])\n",
        "sample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]\n",
        "\n",
        "print(sample_row)\n",
        "print('---')\n",
        "print('Label:', sample_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pr-5FGYzdFY",
        "outputId": "9410c240-ed9c-4ae0-e5a9-d7813ce8c073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Need info on 88-89 Bonneville\n",
            "\n",
            "\n",
            " I am a little confused on all of the models of the 88-89 bonnevilles.\n",
            "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
            "differences are far as features or performance. I am also curious to\n",
            "know what the book value is for prefereably the 89 model. And how much\n",
            "less than book value can you usually get them for. In other words how\n",
            "much are they in demand this time of year. I have heard that the mid-spring\n",
            "early summer is the best time to buy.\n",
            "\n",
            "\t\t\tNeil Gandler\n",
            "\n",
            "---\n",
            "Label: rec.autos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=sample_row)\n"
      ],
      "metadata": {
        "id": "KyzVOlJEzjVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-XQwt0yz9ap",
        "outputId": "3a25f734-d17f-4b1b-fa48-9eb7a862f5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hello Neil,\n\nIt's understandable why you're a little confused about the 1988-1989 Bonneville models! Pontiac had a somewhat complex naming scheme, especially as they started introducing their \"performance luxury\" trims. Let's break it down:\n\nFirst and foremost, a crucial point: **For the 1988-1989 model years, there was NO factory supercharged Bonneville.** The supercharged 3.8L engine (L67) and the SSEi trim didn't arrive for the Bonneville until the 1992 model year. All 1988-1989 Bonnevilles came with the naturally aspirated (non-supercharged) 3.8L V6 engine (the LN3 version of the Buick 3800), making around 165 horsepower and 210 lb-ft of torque, paired with a 4-speed automatic transmission (4T60).\n\nThe differences between the models during these years were primarily in **features, suspension tuning, and aesthetics**, not engine performance. The Bonneville was built on GM's H-body platform, sharing its underpinnings with the Buick LeSabre and Oldsmobile Delta 88.\n\nHere's a breakdown of the models you mentioned:\n\n---\n\n### 1988-1989 Pontiac Bonneville Model Differences:\n\n1.  **Bonneville LE (Luxury Edition):**\n    *   **Position:** The base model.\n    *   **Features:** Focused on comfort and basic amenities. Standard cloth interior, basic instrumentation, power windows/locks (likely optional or standard depending on year/region), standard comfort-tuned suspension, steel wheels with hubcaps. This was designed for general family transportation.\n    *   **Performance:** Same 3.8L NA V6 as all others, but with the softest suspension tuning, it emphasized ride quality over handling.\n\n2.  **Bonneville SE (Sport Edition):**\n    *   **Position:** A step up from the LE, adding a sportier feel and more features.\n    *   **Features:** Included most LE features plus sport-tuned suspension (firmer than LE), usually aluminum alloy wheels (often 15-inch), slightly more aggressive exterior styling elements (mild body cladding or specific grille), upgraded cloth or optional leather interior, more comprehensive instrumentation, and more standard convenience features (e.g., power driver's seat, better audio system).\n    *   **Performance:** Same engine, but the firmer suspension gave it slightly better handling and a sportier feel on the road.\n\n3.  **Bonneville SSE (Special Sport Edition):**\n    *   **Position:** The top-of-the-line performance-luxury model for these years. This was Pontiac's technological flagship for the Bonneville line.\n    *   **Features:** This model was distinctive. It had a very aggressive factory body kit (ground effects, unique bumpers), a rear spoiler, unique 16-inch alloy wheels, and a significantly upgraded interior. Key features included:\n        *   **Full Digital Dash:** A sophisticated digital instrument cluster was standard.\n        *   **Steering Wheel Controls:** Controls for audio, climate, and trip computer integrated into the steering wheel (a big deal for the era).\n        *   **Performance Suspension:** The firmest and most performance-oriented suspension tuning of the trio.\n        *   **Sport Seats:** High-bolstered seats, often in special cloth or leather, designed for lateral support.\n        *   **Premium Audio:** Higher-end sound system was usually standard.\n        *   **Head-Up Display (HUD):** Could be an option or standard on later 1st-gen SSEs, projecting speed onto the windshield.\n        *   **Trip Computer:** Comprehensive onboard computer.\n        *   **Heated Mirrors:** Often standard.\n    *   **Performance:** While it used the same naturally aspirated 3.8L V6, the stiffer suspension, larger wheels/tires, and overall tuning made the SSE feel significantly more athletic and engaging to drive than the LE or SE. It was truly a \"sport sedan\" for its time, just without forced induction.\n\n4.  **Bonneville LSE:**\n    *   The LSE (Luxury Sport Edition) was a very limited production model, and more commonly seen in the early 1990s (around 1990-1991) as a sort of mid-range luxury/sport package, bridging the gap between SE and SSE. It wasn't a standard, full-time trim for the 1988-1989 model years. If you encountered one from '88-89, it might have been a special regional package or a mislabeled car. Focus on the LE, SE, and SSE for these specific years.\n\n5.  **Bonneville SSEi:**\n    *   **As mentioned, the SSEi (Special Sport Edition - *intercooled*) did NOT exist for the 1988-1989 Bonneville.** This model, featuring the supercharged and intercooled 3.8L V6 (L67), debuted for the 1992 model year and was the top-tier performance model from then on.\n\n---\n\n### Book Value for 1989 Bonneville & Demand:\n\nGiven that these cars are now 35 years old, \"book value\" from sources like Kelley Blue Book (KBB) or NADAguides will be extremely low, often reflecting a trade-in value more than a true private-party sale price.\n\n*   **General Range (1989):**\n    *   **LE/SE:** Expect anywhere from **$500 to $1,500** for a decent running example with average mileage. A car needing significant work could be $0-$300. An exceptionally clean, low-mileage example might fetch up to $2,000, but these are rare.\n    *   **SSE:** These command a slight premium due to their unique features and relative rarity. A well-maintained, low-mileage SSE in excellent condition could go for **$1,500 to $3,000**, with exceptional, collector-grade examples potentially reaching $4,000-$5,000 (but these are extremely rare finds). Most will be in the $800-$2,000 range.\n\n*   **\"How much less than book value?\"**\n    *   For cars this old, \"book value\" is often just a starting point and sometimes unrealistically low, especially for private sellers who have invested in maintenance. The actual selling price will almost entirely depend on the **car's physical and mechanical condition, mileage, and maintenance history**.\n    *   If a car is listed above its \"book value,\" it's usually because the seller believes its condition justifies it. If it's listed at or below \"book,\" it might be a quick sale or it needs work.\n    *   **Always prioritize condition over book value for a car of this age.** A $2,500 SSE that runs perfectly and looks great is a much better deal than a \"bargain\" $800 SSE that needs $3,000 in repairs.\n\n*   **Demand:**\n    *   **LE/SE:** Demand is very low. These are generally viewed as older, practical transportation cars, often bought as cheap second cars or for new drivers. They are not sought after by collectors.\n    *   **SSE:** Demand is slightly higher, but still niche. There's a small but dedicated enthusiast following for the early SSEs due to their distinctive styling and advanced features for the era. A well-preserved SSE will sell faster and for more than an LE/SE.\n\n*   **Time of Year (Mid-spring early summer):**\n    *   While this can sometimes be true for newer, sportier cars (people want convertibles or fun cars when the weather is nice), for cars of this age, the time of year is less of a factor than **condition and price**.\n    *   However, good weather in spring/summer *does* make it easier to inspect a car thoroughly (no frozen fluids, easier to spot rust or body damage in good light). It also means more people might be out looking for cars in general. So, while it's not a huge factor, it's not a bad time to look.\n\n**In summary:** If you're looking for a 1988-1989 Bonneville, the **SSE** is the model to seek out for the most features, unique styling, and best driving dynamics (relative to its brethren). Just remember that none of them were supercharged from the factory in those years. Focus on finding a well-maintained example, as mechanical issues can quickly make a cheap car very expensive. Good luck with your search!"
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"From what newsgroup does the following message originate?\"\n",
        "baseline_response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=[prompt, sample_row])\n",
        "print(baseline_response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROtN1y_O0XzL",
        "outputId": "2dd26cd9-0086-4829-e284-82bcd5dfc960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The message originates from the newsgroup **rec.autos**.\n",
            "\n",
            "This newsgroup was, and still is, a common forum for general discussions about automobiles, including specific models, buying advice, technical questions, and comparisons.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "# You can use a system instruction to do more direct prompting, and get a\n",
        "# more succinct answer.\n",
        "\n",
        "system_instruct = \"\"\"\n",
        "You are a classification service. You will be passed input that represents\n",
        "a newsgroup post and you must respond with the newsgroup from which the post\n",
        "originates.\n",
        "\"\"\"\n",
        "\n",
        "# Define a helper to retry when per-minute quota is reached.\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "# If you want to evaluate your own technique, replace this body of this function\n",
        "# with your model, prompt and other code and return the predicted answer.\n",
        "@retry.Retry(predicate=is_retriable)\n",
        "def predict_label(post: str) -> str:\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        config=types.GenerateContentConfig(\n",
        "            system_instruction=system_instruct),\n",
        "        contents=post)\n",
        "\n",
        "    rc = response.candidates[0]\n",
        "\n",
        "    # Any errors, filters, recitation, etc we can mark as a general error\n",
        "    if rc.finish_reason.name != \"STOP\":\n",
        "        return \"(error)\"\n",
        "    else:\n",
        "        # Clean up the response.\n",
        "        return response.text.strip()\n",
        "\n",
        "\n",
        "prediction = predict_label(sample_row)\n",
        "\n",
        "print(prediction)\n",
        "print()\n",
        "print(\"Correct!\" if prediction == sample_label else \"Incorrect.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_aGLvRf9LLS",
        "outputId": "c58a4e11-7910-4548-9ed6-4900c77e7127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rec.autos\n",
            "\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from tqdm.rich import tqdm as tqdmr\n",
        "import warnings\n",
        "\n",
        "# Enable tqdm features on Pandas.\n",
        "tqdmr.pandas()\n",
        "\n",
        "# But suppress the experimental warning\n",
        "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n",
        "\n",
        "\n",
        "# Further sample the test data to be mindful of the free-tier quota.\n",
        "df_baseline_eval = sample_data(df_test, 2, '.*')\n",
        "\n",
        "# Make predictions using the sampled data.\n",
        "df_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)\n",
        "\n",
        "# And calculate the accuracy.\n",
        "accuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum() / len(df_baseline_eval)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50,
          "referenced_widgets": [
            "5a44aaa4cc87462b9499fa727ed91797",
            "c4f52671cc2e4097b2ff8d7f3105711c"
          ]
        },
        "id": "2tDQwNtT9S9k",
        "outputId": "6d179a15-4ad6-4575-bcd7-60026777925a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a44aaa4cc87462b9499fa727ed91797"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_baseline_eval\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2CeQckX9q82",
        "outputId": "9f921d6e-4b06-46b6-8f1c-053e86b69c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Text  Label  \\\n",
              "0   Re: The 1994 Mustang\\n\\n (vlasis theodore) wri...      7   \n",
              "1   Re: Dumbest automotive concepts of all time\\n\\...      7   \n",
              "2   Re: VFR + ST11 Owners get hidden feature\\n\\nIn...      8   \n",
              "3   Re: Your opinion and what it means to me.\\n\\nC...      8   \n",
              "4   Re: Some baseball trivia\\n\\nIn article <>  (Da...      9   \n",
              "5   Re: Jack Morris\\n\\n (John Franjione) writes:\\n...      9   \n",
              "6   Re: German audience is lunatic??\\n\\nIn article...     10   \n",
              "7   Re: SHARKS REVIEW Part 5: Left Wings\\n\\nIn art...     10   \n",
              "8   Re: freely distributable public key cryptograp...     11   \n",
              "9   Official Secrets act (USA)\\n\\nSorry, my news r...     11   \n",
              "10  Re: FM Transmitter ICs- Help!!!!!\\n\\nIn articl...     12   \n",
              "11  Anyone build anything interesting with PIC16C5...     12   \n",
              "12  Re: Candida(yeast) Bloom, Fact or Fiction\\n\\nI...     13   \n",
              "13  Re: Need info on Circumcision, medical cons an...     13   \n",
              "14  Re: Interesting DC-X cost anecdote\\n\\nIn artic...     14   \n",
              "15  Re: moon image in weather sat image\\n\\nIn arti...     14   \n",
              "\n",
              "            Class Name            Prediction  \n",
              "0            rec.autos             rec.autos  \n",
              "1            rec.autos           `rec.autos`  \n",
              "2      rec.motorcycles       rec.motorcycles  \n",
              "3      rec.motorcycles       rec.motorcycles  \n",
              "4   rec.sport.baseball  `rec.sport.baseball`  \n",
              "5   rec.sport.baseball    rec.sport.baseball  \n",
              "6     rec.sport.hockey        rec.sport.misc  \n",
              "7     rec.sport.hockey      rec.sport.hockey  \n",
              "8            sci.crypt             sci.crypt  \n",
              "9            sci.crypt  talk.politics.crypto  \n",
              "10     sci.electronics       soc.electronics  \n",
              "11     sci.electronics  `comp.arch.embedded`  \n",
              "12             sci.med               sci.med  \n",
              "13             sci.med               sci.med  \n",
              "14           sci.space      sci.space.policy  \n",
              "15           sci.space       sci.geo.weather  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-db98d971-55dc-4cf3-befd-66bdb28edada\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "      <th>Class Name</th>\n",
              "      <th>Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Re: The 1994 Mustang\\n\\n (vlasis theodore) wri...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Re: Dumbest automotive concepts of all time\\n\\...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "      <td>`rec.autos`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Re: VFR + ST11 Owners get hidden feature\\n\\nIn...</td>\n",
              "      <td>8</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Re: Your opinion and what it means to me.\\n\\nC...</td>\n",
              "      <td>8</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Re: Some baseball trivia\\n\\nIn article &lt;&gt;  (Da...</td>\n",
              "      <td>9</td>\n",
              "      <td>rec.sport.baseball</td>\n",
              "      <td>`rec.sport.baseball`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Re: Jack Morris\\n\\n (John Franjione) writes:\\n...</td>\n",
              "      <td>9</td>\n",
              "      <td>rec.sport.baseball</td>\n",
              "      <td>rec.sport.baseball</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Re: German audience is lunatic??\\n\\nIn article...</td>\n",
              "      <td>10</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "      <td>rec.sport.misc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Re: SHARKS REVIEW Part 5: Left Wings\\n\\nIn art...</td>\n",
              "      <td>10</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Re: freely distributable public key cryptograp...</td>\n",
              "      <td>11</td>\n",
              "      <td>sci.crypt</td>\n",
              "      <td>sci.crypt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Official Secrets act (USA)\\n\\nSorry, my news r...</td>\n",
              "      <td>11</td>\n",
              "      <td>sci.crypt</td>\n",
              "      <td>talk.politics.crypto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Re: FM Transmitter ICs- Help!!!!!\\n\\nIn articl...</td>\n",
              "      <td>12</td>\n",
              "      <td>sci.electronics</td>\n",
              "      <td>soc.electronics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Anyone build anything interesting with PIC16C5...</td>\n",
              "      <td>12</td>\n",
              "      <td>sci.electronics</td>\n",
              "      <td>`comp.arch.embedded`</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Re: Candida(yeast) Bloom, Fact or Fiction\\n\\nI...</td>\n",
              "      <td>13</td>\n",
              "      <td>sci.med</td>\n",
              "      <td>sci.med</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Re: Need info on Circumcision, medical cons an...</td>\n",
              "      <td>13</td>\n",
              "      <td>sci.med</td>\n",
              "      <td>sci.med</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Re: Interesting DC-X cost anecdote\\n\\nIn artic...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "      <td>sci.space.policy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Re: moon image in weather sat image\\n\\nIn arti...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "      <td>sci.geo.weather</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db98d971-55dc-4cf3-befd-66bdb28edada')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-db98d971-55dc-4cf3-befd-66bdb28edada button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-db98d971-55dc-4cf3-befd-66bdb28edada');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2c98111c-b0f8-4c02-8084-5a134bc28dc7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c98111c-b0f8-4c02-8084-5a134bc28dc7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2c98111c-b0f8-4c02-8084-5a134bc28dc7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_1bc68b91-9d24-478d-a3d0-ee7125a259f7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_baseline_eval')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1bc68b91-9d24-478d-a3d0-ee7125a259f7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_baseline_eval');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_baseline_eval",
              "summary": "{\n  \"name\": \"df_baseline_eval\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"Re: The 1994 Mustang\\n\\n (vlasis theodore) writes:\\n\\n>Have you Detroit beings compared the ultra-long-throw stick shifts of\\n>the 5.0 with the 93 MR2 turbo or 93 RX7 (I ll buy it in 6 mos) ?\\n\\n>Or the Torsen differential of the RX7 compared to the Differential of \\n>the 5.0 that sounds in every hairpin turn ?\\n\\nOr the price tag of the RX7 vs. a Mustang? Part of the definition of\\na Mustang is that it should be affordable by the masses. Of course\\nFord knows youre argument, THEY OWN A BIG PIECE OF MAZDA! Take a good\\nlook at a Mach III, now an RX7, hhhmmmmm...\\n\\n>And bythe way 5.0 and Camaro both have drums on the rear breaks ...\\n>Hello , this is the 90 's ?\\n\\nThat is a tragedy, but I don't think new Camaros or the new Mustangs will.\\n\\n-Steve\\n\\n7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\\n               Alan Kulwicki    1992 Winston Cup Champion\\n                              1954 - 1993\\n7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\\n\\n\\n\",\n          \"Re: Dumbest automotive concepts of all time\\n\\nIn article <>  (Ron Bense) writes:\\n>In article <>  (ALOIS M. HIMSL) writes:\\n>\\n>>In article <>,  (Nancy\\n>> Feagans) writes:\\n>>>Ashtrays and cigarette lighters.  These should be an *option*.\\n>\\n>>You forget that the cigarette lighter plug is essential for plugging in radar\\n>>detectors and lights. The ashtrays are also essential because they are great\\n>>places to keep change and tokens.\\n>\\n>Wouldn't you rather have some type of standard \\\"electrical\\\" plug instead of \\n>that \\\"fire hazard waiting to happen\\\" adaptor? I know I would, and I would \\n>also prefer to have sensibly placed cup holders instead of an ashtray. (my \\n>car came with coin holders already built in)\\n>\\n>Ron\\n\\nExactly.  You took the words right out of my mouth, Ron :-)\\n\\n-- \\n\\nNancy J. Feagans     (818) 306-6423\\nJet Propulsion Lab   \\n\\\"Not a shred of evidence exists in favor of the idea that life is serious.\\\"\\n\",\n          \"Re: Jack Morris\\n\\n (John Franjione) writes:\\n\\n>>-Valentine\\n>>(No, I'm not going to be cordial.  Roger Maynard is a complete and\\n>>total dickhead.  Send me e-mail if you insist on details.)\\n>\\n>In fact, he's a complete and total dickhead on at least 2 newsgroups\\n>(this one and rec.sport.hockey).  Since hockey season is almost over,\\n>he's back to being a dickhead in r.s.bb.\\n\\nI was in fact going to suggest that Roger take his way of discussion over\\nto r.s.football.pro. There this kind of hormone-only reasoning is the\\nstandard. Being he canadian, and hockey what it is, I would have suggested\\nthat r.s.h would work too. It is important in a thread that everyone\\ninvolved use the same body part to produce a post (brain being the organ\\nof choice here).\\n\\nG. Bonvicini\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 7,\n        \"max\": 14,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          8,\n          12,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Class Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"rec.motorcycles\",\n          \"sci.electronics\",\n          \"rec.autos\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"soc.electronics\",\n          \"sci.med\",\n          \"rec.autos\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Iterable\n",
        "import random\n",
        "\n",
        "\n",
        "input_data = {'examples':\n",
        "    df_train[['Text', 'Class Name']]\n",
        "      .rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n",
        "      .to_dict(orient='records')\n",
        " }\n",
        "\n",
        "model_id = None\n",
        "\n",
        "if not model_id:\n",
        "  queued_model = None\n",
        "  for m in reversed(client.tunings.list()):\n",
        "    if m.name.startswith('tunedModels/newsgroup-classification-model'):\n",
        "      if m.state.name == 'JOB_STATE_SUCCEEDED':\n",
        "        model_id = m.name\n",
        "        print('Found existing tuned model to reuse.')\n",
        "        break\n",
        "      elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n",
        "        # If there's a model still queued, remember the most recent one.\n",
        "        queued_model = m.name\n",
        "  else:\n",
        "    if queued_model:\n",
        "      model_id = queued_model\n",
        "      print('Found queued model, still waiting.')\n",
        "\n",
        "\n",
        "if not model_id:\n",
        "    tuning_op = client.tunings.tune(\n",
        "        base_model=\"models/gemini-2.5-flash-001\",\n",
        "        training_dataset=input_data,\n",
        "        config=types.CreateTuningJobConfig(\n",
        "            tuned_model_display_name=\"Newsgroup classification model\",\n",
        "            batch_size=16,\n",
        "            epoch_count=2,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    print(tuning_op.state)\n",
        "    model_id = tuning_op.name\n",
        "\n",
        "print(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "eiCBTVFw_eFy",
        "outputId": "f7852d18-1197-4fe6-92a8-a5801f6f63c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2134991861.py:31: ExperimentalWarning: The SDK's tuning implementation is experimental, and may change in future versions.\n",
            "  tuning_op = client.tunings.tune(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': '* CreateTunedModelRequest.tuned_model.tuning_task.training_data: missing field.\\n', 'status': 'INVALID_ARGUMENT'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2134991861.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     tuning_op = client.tunings.tune(\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mbase_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models/gemini-2.5-flash-001\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_common.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         )\n\u001b[0;32m--> 692\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/tunings.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, base_model, training_dataset, config)\u001b[0m\n\u001b[1;32m   1610\u001b[0m           \u001b[0mtuning_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidated_evaluation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m       operation = self._tune_mldev(\n\u001b[0m\u001b[1;32m   1613\u001b[0m           \u001b[0mbase_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m           \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/tunings.py\u001b[0m in \u001b[0;36m_tune_mldev\u001b[0;34m(self, base_model, pre_tuned_model, training_dataset, config)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   1505\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[0;32m-> 1331\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m     response_body = (\n\u001b[1;32m   1333\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m   async def _async_request_once(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1142\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m       )\n\u001b[0;32m-> 1144\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m       return HttpResponse(\n\u001b[1;32m   1146\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': '* CreateTunedModelRequest.tuned_model.tuning_task.training_data: missing field.\\n', 'status': 'INVALID_ARGUMENT'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready: convert df to JSONL and optionally upload to GCS\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Step 1 — create JSONL locally\n",
        "records = (\n",
        "    df_train[['Text', 'Class Name']]\n",
        "    .rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n",
        "    .to_dict(orient='records')\n",
        ")\n",
        "\n",
        "jsonl_path = \"train.jsonl\"\n",
        "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for rec in records:\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Wrote {len(records)} records to {jsonl_path}\")\n",
        "\n",
        "# ---------- Option A: If your client accepts a local file path ----------\n",
        "# Some client libraries let you pass a filepath or a file-like object.\n",
        "# Try this if the API docs say \"upload file\" or accept a path.\n",
        "try:\n",
        "    if not model_id:\n",
        "        tuning_op = client.tunings.tune(\n",
        "            base_model=\"models/gemini-2.5-flash\",\n",
        "            training_dataset=jsonl_path,  # <-- local path (only if supported)\n",
        "            config=types.CreateTuningJobConfig(\n",
        "                tuned_model_display_name=\"Newsgroup classification model\",\n",
        "                batch_size=16,\n",
        "                epoch_count=2,\n",
        "            ),\n",
        "        )\n",
        "        print(\"Tuning job submitted (using local path).\")\n",
        "except Exception as e:\n",
        "    print(\"Local-path submission failed (likely unsupported). Error:\", e)\n",
        "\n",
        "# ---------- Option B: Upload JSONL to Google Cloud Storage (recommended) ----------\n",
        "# Many tuning endpoints expect a cloud URI (gs://...). This uses google-cloud-storage.\n",
        "# You need to run `pip install --upgrade google-cloud-storage` in Colab if missing,\n",
        "# and have authenticated Colab (gcloud auth login or use Colab's auth).\n",
        "#\n",
        "# Replace YOUR_BUCKET with your GCS bucket name.\n",
        "\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    from google.cloud import storage\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket_name = \"YOUR_BUCKET\"   # <<-- change this\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(os.path.basename(jsonl_path))\n",
        "    blob.upload_from_filename(jsonl_path)\n",
        "    gcs_uri = f\"gs://{bucket_name}/{blob.name}\"\n",
        "    print(\"Uploaded to:\", gcs_uri)\n",
        "\n",
        "    # Now call the tuning API with the GCS URI.\n",
        "    # Many APIs accept a dict like {\"gcs_uri\": \"...\"} or training_dataset=gcs_uri.\n",
        "    # Check your API docs; below are two common patterns — try the one your client expects.\n",
        "\n",
        "    # Pattern 1: pass the GCS URI as a string\n",
        "    try:\n",
        "        if not model_id:\n",
        "            tuning_op = client.tunings.tune(\n",
        "                base_model=\"models/gemini-2.5-flash\",\n",
        "                training_dataset=gcs_uri,  # some clients accept a gs:// path\n",
        "                config=types.CreateTuningJobConfig(\n",
        "                    tuned_model_display_name=\"Newsgroup classification model\",\n",
        "                    batch_size=16,\n",
        "                    epoch_count=2,\n",
        "                ),\n",
        "            )\n",
        "            print(\"Tuning job submitted (using GCS URI).\")\n",
        "    except Exception as e:\n",
        "        print(\"Submission with plain GCS URI failed:\", e)\n",
        "\n",
        "    # Pattern 2: pass an object referencing the file (API-specific)\n",
        "    try:\n",
        "        if not model_id:\n",
        "            # Example shape: adapt to your client. Many APIs want {\"file_uri\": \"...\"} or {\"gcs_uri\": \"...\"}\n",
        "            training_dataset_ref = {\"gcs_uri\": gcs_uri}\n",
        "            tuning_op = client.tunings.tune(\n",
        "                base_model=\"models/gemini-2.5-flash\",\n",
        "                training_dataset=training_dataset_ref,\n",
        "                config=types.CreateTuningJobConfig(\n",
        "                    tuned_model_display_name=\"Newsgroup classification model\",\n",
        "                    batch_size=16,\n",
        "                    epoch_count=2,\n",
        "                ),\n",
        "            )\n",
        "            print(\"Tuning job submitted (using training_dataset object).\")\n",
        "    except Exception as e:\n",
        "        print(\"Submission with training_dataset object failed:\", e)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"GCS upload block failed. If you don't want to use GCS, skip Option B. Error:\", e)\n"
      ],
      "metadata": {
        "id": "FHITJYKLQC0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = client.models.list()\n",
        "for model in models:\n",
        "  print(model.name)"
      ],
      "metadata": {
        "id": "1PKBebcqIqFl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}