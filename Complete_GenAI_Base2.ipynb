{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp39Cm98MQ/m25sqC3gwmM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parshvak26/GENAI/blob/main/Complete_GenAI_Base2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E1pICdJNQM8"
      },
      "outputs": [],
      "source": [
        "# !pip install -U google-genai>=1.37.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "e48hX8yFO-lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)\n"
      ],
      "metadata": {
        "id": "xc5s28y-PDYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YourAPIKEY\"\n"
      ],
      "metadata": {
        "id": "w4MeSpouPKAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Explain AI to me like I'm a kid.\"\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "id": "NpTAA3w2PnZJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "id": "rudKuPzMSWMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
        "response = chat.send_message('Hello! My name is Parshva. I am just starting with GENAI. Any tips?')\n"
      ],
      "metadata": {
        "id": "l5o5QhTLScN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "g1sXvAS6TwsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "id": "sHf8fArfT0vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == 'models/gemini-2.5-flash':\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "id": "h5JQSqdeT6kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Temperature**\n",
        "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
        "\n",
        "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
      ],
      "metadata": {
        "id": "181T6C1uUrNy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXQ8xqSJc3wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
        "# high_temp_config = types.GenerateContentConfig(temperature=0.0) #This is Low temperature. Try uncommenting above one and see change in output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for _ in range(5):\n",
        "#   response = client.models.generate_content(\n",
        "#       model='gemini-2.5-flash',\n",
        "#       config=high_temp_config,\n",
        "#       contents='Pick a random colour... (respond in a single word)')\n",
        "\n",
        "#   if response.text:\n",
        "#     print(response.text, '-' * 25)"
      ],
      "metadata": {
        "id": "h3t2uUf8T_LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Top-P**\n",
        "Like temperature, the top-P parameter is also used to control the diversity of the model's output.\n",
        "\n",
        "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
        "\n",
        "You may also see top-K referenced in LLM literature. Top-K is not configurable in the Gemini 2.0 series of models, but can be changed in older models. Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
        "\n",
        "Run this example a number of times, change the settings and observe the change in output."
      ],
      "metadata": {
        "id": "o9Wj5K_QVIBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_config = types.GenerateContentConfig(\n",
        "#     # These are the default values for gemini-2.0-flash.\n",
        "#     temperature=1.0,\n",
        "#     top_p=0.95,\n",
        "# )\n",
        "\n",
        "# story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=model_config,\n",
        "#     contents=story_prompt)\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "rU_Lo9neVTBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompting**"
      ],
      "metadata": {
        "id": "0vqYrp65XhDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Zero Shot**\n",
        "\n",
        "Zero-shot prompts are prompts that describe the request for the model directly.\n",
        "\n",
        "**EXAMPLE**- Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment:"
      ],
      "metadata": {
        "id": "u0ExNwrcXmOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enum mode**\n",
        "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards."
      ],
      "metadata": {
        "id": "sZc118JWYCAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM**, short for Enumeration, is basically a fancy way of saying “a list of named options you can choose from.” It’s not some secret AI spell — it’s just a data structure used in programming (and yes, also in GenAI frameworks) to define a fixed set of values that something can take.\n",
        "\n",
        "Think of it like giving names to a few specific choices instead of letting someone type random junk. It’s like saying:\n",
        "\n",
        "“You can pick from {Cat, Dog, Hamster}, but not ‘DragonWithWiFi’.”"
      ],
      "metadata": {
        "id": "Bfz7sc61adDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you’re designing a GenAI pipeline where the model can only act in certain modes:\n",
        "\n",
        "    class AgentAction(Enum):\n",
        "\n",
        "      SUMMARIZE = \"summarize\"\n",
        "\n",
        "      TRANSLATE = \"translate\"\n",
        "\n",
        "      CODE = \"code\"\n",
        "\n",
        "\n",
        "So, if the model gets “SUMMARIZE”, it knows exactly what operation to perform. Keeps things neat, predictable, and stops the AI from hallucinating another mode called “make memes”."
      ],
      "metadata": {
        "id": "0qKInIFzarv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import enum\n",
        "\n",
        "# class Sentiment(enum.Enum):\n",
        "#     POSITIVE = \"positive\"\n",
        "#     NEUTRAL = \"neutral\"\n",
        "#     NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         response_mime_type=\"text/x.enum\",\n",
        "#         response_schema=Sentiment\n",
        "#     ),\n",
        "#     contents=zero_shot_prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "### OUTPUT would be \"positive\"\n",
        "\n",
        "\n",
        "# enum_response = response.parsed\n",
        "# print(enum_response)\n",
        "# print(type(enum_response))"
      ],
      "metadata": {
        "id": "g6m1c-YZXkAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENUM** = a controlled vocabulary for your GenAI system.\n",
        "It ensures structure and sanity in a world full of chaotic data and even more chaotic humans.\n",
        "\n",
        "As Seneca said, “Order is what keeps the universe from sliding into chaos.”"
      ],
      "metadata": {
        "id": "xy9VlGWia6db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One-shot and few-shot**\n",
        "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt."
      ],
      "metadata": {
        "id": "Qpnxrr_sdE8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "# customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.5-flash',\n",
        "#     config=types.GenerateContentConfig(\n",
        "#         temperature=0.1,\n",
        "#         top_p=1,\n",
        "#         max_output_tokens=250,\n",
        "#     ),\n",
        "#     contents=[few_shot_prompt, customer_order])\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "727sAItvYMAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chain of Thought (CoT)**\n",
        "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
        "\n",
        "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
        "\n",
        "Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try."
      ],
      "metadata": {
        "id": "VQX-v-yljc0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "# am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# print(response.text)\n",
        "\n",
        "# Output -\n",
        "# 52"
      ],
      "metadata": {
        "id": "h8rsJjxZjf6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "# I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "# response = client.models.generate_content(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     contents=prompt)\n",
        "\n",
        "# Markdown(response.text)\n",
        "\n",
        "# Output-\n",
        "# Here's how to solve this:\n",
        "\n",
        "# Find the age difference: When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
        "\n",
        "# Calculate the age difference: The age difference between you and your partner is 12 - 4 = 8 years.\n",
        "\n",
        "# Determine partner's current age: Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
        "\n",
        "# Answer: Your partner is currently 28 years old."
      ],
      "metadata": {
        "id": "wkUWbxxrjmKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ReAct: Reason and act**\n",
        "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper."
      ],
      "metadata": {
        "id": "hQfRkvf4kuln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of an AI model just thinking (reasoning) or just doing (acting), ReAct makes it do both — in turns.\n",
        "\n",
        "The model reasons about the problem step-by-step, takes an action (like calling a tool, searching, or retrieving info), observes the result, and then continues reasoning.\n",
        "Think of it like a detective alternating between thinking out loud and doing stuff until the mystery’s solved."
      ],
      "metadata": {
        "id": "Hi5urWTwpWnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Step-by-step*\n",
        "\n",
        "Reasoning → the model thinks: “Hmm, what do I need to solve this?”\n",
        "\n",
        "Acting → it takes an action: “Let me look that up in the database.”\n",
        "\n",
        "Observation → it gets the result: “Okay, here’s what I found.”\n",
        "\n",
        "Repeat → it continues reasoning with the new info until it reaches the answer.\n",
        "\n",
        "So instead of blindly guessing, the model becomes something like a thoughtful agent that mixes logic with action — like a data scientist who actually tests their hypothesis instead of just tweeting it."
      ],
      "metadata": {
        "id": "wCXMNdF4pu1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Question - “What’s the current temperature in New York?”\n",
        "\n",
        "    The AI’s internal process might look like this:\n",
        "\n",
        "      Thought: I need real-time weather data.\n",
        "\n",
        "      Action: call_weather_api(\"New York\")\n",
        "\n",
        "      Observation: The API returns 12°C.\n",
        "\n",
        "      Thought: The temperature in New York is 12°C.\n",
        "\n",
        "      Final Answer: It’s 12°C in New York."
      ],
      "metadata": {
        "id": "8Z8iiLlcp_gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_instructions = \"\"\"\n",
        "# Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "# Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        "#  (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "#      will return some similar entities to search and you can try to search the information from those topics.\n",
        "#  (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "#      so keep your searches short.\n",
        "#  (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "# \"\"\"\n",
        "\n",
        "# example1 = \"\"\"Question\n",
        "# Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "# Thought 1\n",
        "# The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "# Action 1\n",
        "# <search>Milhouse</search>\n",
        "# Observation 1\n",
        "# Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "# Thought 2\n",
        "# The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "# Action 2\n",
        "# <lookup>named after</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "# Thought 3\n",
        "# Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "# Action 3\n",
        "# <finish>Richard Nixon</finish>\n",
        "# \"\"\"\n",
        "\n",
        "# example2 = \"\"\"Question\n",
        "# What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "# Thought 1\n",
        "# I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "# Action 1\n",
        "# <search>Colorado orogeny</search>\n",
        "\n",
        "# Observation 1\n",
        "# The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "# Thought 2\n",
        "# It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "# Action 2\n",
        "# <lookup>eastern sector</lookup>\n",
        "\n",
        "# Observation 2\n",
        "# The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "# Thought 3\n",
        "# The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "# Action 3\n",
        "# <search>High Plains</search>\n",
        "\n",
        "# Observation 3\n",
        "# High Plains refers to one of two distinct land regions\n",
        "\n",
        "# Thought 4\n",
        "# I need to instead search High Plains (United States).\n",
        "\n",
        "# Action 4\n",
        "# <search>High Plains (United States)</search>\n",
        "\n",
        "# Observation 4\n",
        "# The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "# Thought 5\n",
        "# High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "# Action 5\n",
        "# <finish>1,800 to 7,000 ft</finish>\n",
        "# \"\"\"\n",
        "# # Take a look through https://github.com/ysymyth/ReAct/\n"
      ],
      "metadata": {
        "id": "eucmY6UHkxpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"\"\"Question\n",
        "# Who was the youngest author listed on the transformers NLP paper?\n",
        "# \"\"\"\n",
        "\n",
        "# # You will perform the Action; so generate up to, but not including, the Observation.\n",
        "# react_config = types.GenerateContentConfig(\n",
        "#     stop_sequences=[\"\\nObservation\"],\n",
        "#     system_instruction=model_instructions + example1 + example2,\n",
        "# )\n",
        "\n",
        "# # Create a chat that has the model instructions and examples pre-seeded.\n",
        "# react_chat = client.chats.create(\n",
        "#     model='gemini-2.0-flash',\n",
        "#     config=react_config,\n",
        "# )\n",
        "\n",
        "# resp = react_chat.send_message(question)\n",
        "# print(resp.text)"
      ],
      "metadata": {
        "id": "vmjC5coRlMiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Thinking mode**\n",
        "The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n",
        "\n",
        "Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated."
      ],
      "metadata": {
        "id": "MSHrgednQP9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "# from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "# response = client.models.generate_content_stream(\n",
        "#     model='gemini-2.0-flash-thinking-exp',\n",
        "#     contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        "# )\n",
        "\n",
        "# buf = io.StringIO()\n",
        "# for chunk in response:\n",
        "#     buf.write(chunk.text)\n",
        "#     # Display the response as it is streamed\n",
        "#     print(chunk.text, end='')\n",
        "\n",
        "# # And then render the finished response as formatted markdown.\n",
        "# clear_output()\n",
        "# Markdown(buf.getvalue())"
      ],
      "metadata": {
        "id": "OF8KliWnQSyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Q&A with RAG using Chroma\n"
      ],
      "metadata": {
        "id": "0nmEocRvRbPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two big limitations of LLMs are 1) that they only \"know\" the information that they were trained on, and 2) that they have limited input context windows. A way to address both of these limitations is to use a technique called Retrieval Augmented Generation, or RAG. A RAG system has three stages:\n",
        "\n",
        "    Indexing\n",
        "    Retrieval\n",
        "    Generation"
      ],
      "metadata": {
        "id": "a2v-GQwBRgh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Indexing → Break documents into chunks, turn them into embeddings (numerical meaning), and store them in a vector database.\n",
        "\n",
        "2. Retrieval → When asked a question, find the most relevant chunks from that database using semantic similarity.\n",
        "\n",
        "3. Generation → Feed those chunks + the question to the LLM so it can craft a grounded, natural-language answer.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  - Documents → [Indexing] → Vector DB\n",
        "\n",
        "  - User Query → [Retrieval] → Top Relevant Chunks\n",
        "\n",
        "  - Query + Chunks → [Generation] → Final Answer"
      ],
      "metadata": {
        "id": "CJjWD7WUTNB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG, short for Retrieval-Augmented Generation, is just a fancy combo move that helps AI answer questions based on real documents"
      ],
      "metadata": {
        "id": "YpwpNRTASoNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal LLMs are good at language but terrible at memory.\n",
        "You ask about some obscure company policy PDF — I can’t know it, because it’s not in my training data.\n",
        "\n",
        "So, RAG fixes that by giving the model access to your specific documents.\n",
        "\n",
        "It works like this:\n",
        "\n",
        "1. Store your documents somewhere searchable (usually as chunks of text in a “vector database” — basically a big mathy filing cabinet).\n",
        "\n",
        "2. When you ask a question, the system:\n",
        "\n",
        "    - Searches for the most relevant parts of those docs.\n",
        "\n",
        "    - Pulls out the top matches.\n",
        "\n",
        "3. Then it feeds both your question and those retrieved snippets into the language model.\n",
        "\n",
        "4. The model uses that context to generate a smart, grounded answer — not a wild guess.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UN6QDvzoSQ9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "    if \"embedContent\" in m.supported_actions:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "id": "nfR8i125Rdt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Data"
      ],
      "metadata": {
        "id": "grN5EbpKVizu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\"\n",
        "DOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.'\n",
        "DOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"\n",
        "\n",
        "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
      ],
      "metadata": {
        "id": "D9Ri1rUdVkyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install chromadb\n"
      ],
      "metadata": {
        "id": "CAuwwEceWG-p",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "# Define a helper to retry when per-minute quota is reached.\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = \"retrieval_document\"\n",
        "        else:\n",
        "            embedding_task = \"retrieval_query\"\n",
        "\n",
        "        response = client.models.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            contents=input,\n",
        "            config=types.EmbedContentConfig(\n",
        "                task_type=embedding_task,\n",
        "            ),\n",
        "        )\n",
        "        return [e.values for e in response.embeddings]"
      ],
      "metadata": {
        "id": "yCOQ3NlfVnXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "DB_NAME = \"googlecardb\"\n",
        "\n",
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
        "\n",
        "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])"
      ],
      "metadata": {
        "id": "iAZsrX2kVwlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.count()\n"
      ],
      "metadata": {
        "id": "Q9tPA-l2XFXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to query mode when generating embeddings.\n",
        "embed_fn.document_mode = False\n",
        "\n",
        "# Search the Chroma DB using the specified query.\n",
        "query = \"How do you use the touchscreen to play music?\"\n",
        "\n",
        "result = db.query(query_texts=[query], n_results=1)\n",
        "[all_passages] = result[\"documents\"]\n",
        "\n",
        "Markdown(all_passages[0])"
      ],
      "metadata": {
        "id": "PCYhVsrxXKjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_passages"
      ],
      "metadata": {
        "id": "dI_KY52ba5Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Augmented generation: Answer the question**\n",
        "\n",
        "Now that we’ve found a relevant passage from our document set during the retrieval step, we can move on to assembling a generation prompt and use the Gemini API to produce the final answer.\n",
        "\n",
        "In this example, we only retrieved a single passage. But in real-world scenarios — especially when dealing with a large collection of data — we’d typically retrieve multiple passages. That way, the Gemini model can decide which pieces of text are actually useful for answering the question.\n",
        "\n",
        "It’s perfectly fine if a few of those retrieved passages aren’t directly relevant; the model’s generation process is designed to filter out the noise and focus on what truly matters."
      ],
      "metadata": {
        "id": "4RUBd5M_aAn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_oneline = query.replace(\"\\n\", \" \")\n",
        "\n",
        "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
        "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "QUESTION: {query_oneline}\n",
        "\"\"\"\n",
        "\n",
        "# Add the retrieved documents to the prompt.\n",
        "for passage in all_passages:\n",
        "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
        "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "_ya2lOdKXhgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(answer.text)"
      ],
      "metadata": {
        "id": "ooOuI5F6atCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning Model"
      ],
      "metadata": {
        "id": "nRo6fpJ3lsI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning means taking a pretrained model (like Gemini, GPT, or Llama) and teaching it new tricks — not from scratch, but by giving it extra, specific examples so it learns your tone, domain, or task.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        " - The base model = a smart intern who knows everything in general.\n",
        "\n",
        " - Fine-tuning = teaching that intern how your company does things.\n",
        "\n",
        "You’re not retraining their whole brain — you’re just nudging their habits."
      ],
      "metadata": {
        "id": "wqw0A3xxl1YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Start with a pretrained model**\n",
        "\n",
        "    You don’t build from zero — you take an existing model like:\n",
        "\n",
        "    text-bison, gemini, gpt-3.5-turbo, etc.\n",
        "    These models already know grammar, logic, reasoning — basically the “universal stuff.”\n",
        "\n",
        "2. **Prepare your custom data**\n",
        "\n",
        "    You create a dataset that shows the model how you want it to behave.\n",
        "    Usually looks like this:\n",
        "\n",
        "        {\"input\": \"What is your refund policy?\", \"output\": \"Refunds are processed in 14 days.\"}\n",
        "  \n",
        "    OR\n",
        "  \n",
        "        {\"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Summarize this report.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Here’s a short summary...\"}\n",
        "        ]}\n",
        "    \n",
        "    You collect hundreds or thousands of such examples. The more clean and consistent, the better.\n",
        "\n",
        "3. **Train the model**\n",
        "\n",
        "    You feed that dataset into the fine-tuning API (e.g., Google’s Vertex AI, OpenAI’s fine-tuning endpoint, etc.).\n",
        "\n",
        "    Under the hood:\n",
        "\n",
        "    - The base weights are slightly adjusted to prefer your examples.\n",
        "\n",
        "    - The model’s general knowledge stays intact.\n",
        "\n",
        "    This step takes anywhere from minutes to hours depending on your data and compute.\n",
        "\n",
        "4. **Deploy your fine-tuned model**\n",
        "\n",
        "    You’ll get a new model ID, something like:\n",
        "\n",
        "        projects/your-id/locations/us-central1/models/fine-tuned-customer-support\n",
        "\n",
        "    Now you use that instead of the base model in your API calls.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "rmG99doNmZbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Use Cases**\n",
        "\n",
        " - Customer support: Model learns your company’s tone, product facts, policies.\n",
        "\n",
        " - Code generation: Teach it your team’s code style, naming conventions.\n",
        "\n",
        " - Legal / medical text: Make it speak your industry’s language.\n",
        "\n",
        " - Creative writing: Train it to write like you, not a Wikipedia article."
      ],
      "metadata": {
        "id": "_Qmi9ypRoo10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Fine-Tuning isn’t**\n",
        "\n",
        " - It’s not for teaching brand-new world knowledge.\n",
        "(You can’t fine-tune GPT on tomorrow’s news — that’s RAG’s job.)\n",
        "\n",
        " - It’s not for fixing hallucinations.\n",
        "It’s for improving style and consistency.\n",
        "\n",
        " - And it’s definitely not cheap — those GPUs are thirsty."
      ],
      "metadata": {
        "id": "nLbzn524o4cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset=\"train\")\n",
        "newsgroups_test = fetch_20newsgroups(subset=\"test\")\n",
        "\n",
        "# View list of class names for dataset\n",
        "newsgroups_train.target_names"
      ],
      "metadata": {
        "id": "i7FjxkNG29VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsgroups_train.data[0])\n"
      ],
      "metadata": {
        "id": "Hx9TfDg7JczQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# newsgroups_train"
      ],
      "metadata": {
        "id": "zb9B5Wp0qeQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'data': [\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\","
      ],
      "metadata": {
        "id": "roFQv25ot_7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_newsgroup_row(data):\n",
        "    # Split headers from body manually\n",
        "    parts = data.split(\"\\n\\n\", 1)\n",
        "    headers = parts[0] if len(parts) > 0 else \"\"\n",
        "    body = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    # Extract the Subject field manually\n",
        "    subject = \"\"\n",
        "    for line in headers.split(\"\\n\"):\n",
        "        if line.lower().startswith(\"subject:\"):\n",
        "            subject = line[len(\"subject:\"):].strip()\n",
        "            break\n",
        "\n",
        "    # Build text\n",
        "    text = f\"{subject}\\n\\n{body}\"\n",
        "\n",
        "    # Strip email addresses\n",
        "    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n",
        "\n",
        "    # Truncate\n",
        "    return text[:40000]\n",
        "\n",
        "\n",
        "def preprocess_newsgroup_data(newsgroup_dataset):\n",
        "    df = pd.DataFrame({\n",
        "        \"Text\": newsgroup_dataset.data,\n",
        "        \"Label\": newsgroup_dataset.target\n",
        "    })\n",
        "\n",
        "    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n",
        "    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "tVHcHLQftjD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = preprocess_newsgroup_data(newsgroups_train)\n",
        "df_test = preprocess_newsgroup_data(newsgroups_test)\n",
        "\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "s5Kxm8F1tmRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_data(df, num_samples, classes_to_keep):\n",
        "    # Sample rows, selecting num_samples of each Label.\n",
        "    df = (\n",
        "        df.groupby(\"Label\")[df.columns]\n",
        "        .apply(lambda x: x.sample(num_samples))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n",
        "    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "TRAIN_NUM_SAMPLES = 50\n",
        "TEST_NUM_SAMPLES = 10\n",
        "# Keep rec.* and sci.*\n",
        "CLASSES_TO_KEEP = \"^rec|^sci\"\n",
        "\n",
        "df_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\n",
        "df_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)\n"
      ],
      "metadata": {
        "id": "ZbCD0QQZytLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0\n",
        "sample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx])\n",
        "sample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]\n",
        "\n",
        "print(sample_row)\n",
        "print('---')\n",
        "print('Label:', sample_label)"
      ],
      "metadata": {
        "id": "3Pr-5FGYzdFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=sample_row)\n"
      ],
      "metadata": {
        "id": "KyzVOlJEzjVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H-XQwt0yz9ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"From what newsgroup does the following message originate?\"\n",
        "baseline_response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=[prompt, sample_row])\n",
        "print(baseline_response.text)"
      ],
      "metadata": {
        "id": "ROtN1y_O0XzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "# You can use a system instruction to do more direct prompting, and get a\n",
        "# more succinct answer.\n",
        "\n",
        "system_instruct = \"\"\"\n",
        "You are a classification service. You will be passed input that represents\n",
        "a newsgroup post and you must respond with the newsgroup from which the post\n",
        "originates.\n",
        "\"\"\"\n",
        "\n",
        "# Define a helper to retry when per-minute quota is reached.\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "# If you want to evaluate your own technique, replace this body of this function\n",
        "# with your model, prompt and other code and return the predicted answer.\n",
        "@retry.Retry(predicate=is_retriable)\n",
        "def predict_label(post: str) -> str:\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        config=types.GenerateContentConfig(\n",
        "            system_instruction=system_instruct),\n",
        "        contents=post)\n",
        "\n",
        "    rc = response.candidates[0]\n",
        "\n",
        "    # Any errors, filters, recitation, etc we can mark as a general error\n",
        "    if rc.finish_reason.name != \"STOP\":\n",
        "        return \"(error)\"\n",
        "    else:\n",
        "        # Clean up the response.\n",
        "        return response.text.strip()\n",
        "\n",
        "\n",
        "prediction = predict_label(sample_row)\n",
        "\n",
        "print(prediction)\n",
        "print()\n",
        "print(\"Correct!\" if prediction == sample_label else \"Incorrect.\")"
      ],
      "metadata": {
        "id": "x_aGLvRf9LLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from tqdm.rich import tqdm as tqdmr\n",
        "import warnings\n",
        "\n",
        "# Enable tqdm features on Pandas.\n",
        "tqdmr.pandas()\n",
        "\n",
        "# But suppress the experimental warning\n",
        "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n",
        "\n",
        "\n",
        "# Further sample the test data to be mindful of the free-tier quota.\n",
        "df_baseline_eval = sample_data(df_test, 2, '.*')\n",
        "\n",
        "# Make predictions using the sampled data.\n",
        "df_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)\n",
        "\n",
        "# And calculate the accuracy.\n",
        "accuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum() / len(df_baseline_eval)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "2tDQwNtT9S9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_baseline_eval\n"
      ],
      "metadata": {
        "id": "N2CeQckX9q82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Iterable\n",
        "import random\n",
        "\n",
        "\n",
        "input_data = {'examples':\n",
        "    df_train[['Text', 'Class Name']]\n",
        "      .rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n",
        "      .to_dict(orient='records')\n",
        " }\n",
        "\n",
        "model_id = None\n",
        "\n",
        "if not model_id:\n",
        "  queued_model = None\n",
        "  for m in reversed(client.tunings.list()):\n",
        "    if m.name.startswith('tunedModels/newsgroup-classification-model'):\n",
        "      if m.state.name == 'JOB_STATE_SUCCEEDED':\n",
        "        model_id = m.name\n",
        "        print('Found existing tuned model to reuse.')\n",
        "        break\n",
        "      elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n",
        "        # If there's a model still queued, remember the most recent one.\n",
        "        queued_model = m.name\n",
        "  else:\n",
        "    if queued_model:\n",
        "      model_id = queued_model\n",
        "      print('Found queued model, still waiting.')\n",
        "\n",
        "\n",
        "if not model_id:\n",
        "    tuning_op = client.tunings.tune(\n",
        "        base_model=\"models/gemini-2.5-flash-001\",\n",
        "        training_dataset=input_data,\n",
        "        config=types.CreateTuningJobConfig(\n",
        "            tuned_model_display_name=\"Newsgroup classification model\",\n",
        "            batch_size=16,\n",
        "            epoch_count=2,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    print(tuning_op.state)\n",
        "    model_id = tuning_op.name\n",
        "\n",
        "print(model_id)"
      ],
      "metadata": {
        "id": "eiCBTVFw_eFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready: convert df to JSONL and optionally upload to GCS\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Step 1 — create JSONL locally\n",
        "records = (\n",
        "    df_train[['Text', 'Class Name']]\n",
        "    .rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n",
        "    .to_dict(orient='records')\n",
        ")\n",
        "\n",
        "jsonl_path = \"train.jsonl\"\n",
        "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for rec in records:\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Wrote {len(records)} records to {jsonl_path}\")\n",
        "\n",
        "# ---------- Option A: If your client accepts a local file path ----------\n",
        "# Some client libraries let you pass a filepath or a file-like object.\n",
        "# Try this if the API docs say \"upload file\" or accept a path.\n",
        "try:\n",
        "    if not model_id:\n",
        "        tuning_op = client.tunings.tune(\n",
        "            base_model=\"models/gemini-2.5-flash\",\n",
        "            training_dataset=jsonl_path,  # <-- local path (only if supported)\n",
        "            config=types.CreateTuningJobConfig(\n",
        "                tuned_model_display_name=\"Newsgroup classification model\",\n",
        "                batch_size=16,\n",
        "                epoch_count=2,\n",
        "            ),\n",
        "        )\n",
        "        print(\"Tuning job submitted (using local path).\")\n",
        "except Exception as e:\n",
        "    print(\"Local-path submission failed (likely unsupported). Error:\", e)\n",
        "\n",
        "# ---------- Option B: Upload JSONL to Google Cloud Storage (recommended) ----------\n",
        "# Many tuning endpoints expect a cloud URI (gs://...). This uses google-cloud-storage.\n",
        "# You need to run `pip install --upgrade google-cloud-storage` in Colab if missing,\n",
        "# and have authenticated Colab (gcloud auth login or use Colab's auth).\n",
        "#\n",
        "# Replace YOUR_BUCKET with your GCS bucket name.\n",
        "\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    from google.cloud import storage\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket_name = \"YOUR_BUCKET\"   # <<-- change this\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(os.path.basename(jsonl_path))\n",
        "    blob.upload_from_filename(jsonl_path)\n",
        "    gcs_uri = f\"gs://{bucket_name}/{blob.name}\"\n",
        "    print(\"Uploaded to:\", gcs_uri)\n",
        "\n",
        "    # Now call the tuning API with the GCS URI.\n",
        "    # Many APIs accept a dict like {\"gcs_uri\": \"...\"} or training_dataset=gcs_uri.\n",
        "    # Check your API docs; below are two common patterns — try the one your client expects.\n",
        "\n",
        "    # Pattern 1: pass the GCS URI as a string\n",
        "    try:\n",
        "        if not model_id:\n",
        "            tuning_op = client.tunings.tune(\n",
        "                base_model=\"models/gemini-2.5-flash\",\n",
        "                training_dataset=gcs_uri,  # some clients accept a gs:// path\n",
        "                config=types.CreateTuningJobConfig(\n",
        "                    tuned_model_display_name=\"Newsgroup classification model\",\n",
        "                    batch_size=16,\n",
        "                    epoch_count=2,\n",
        "                ),\n",
        "            )\n",
        "            print(\"Tuning job submitted (using GCS URI).\")\n",
        "    except Exception as e:\n",
        "        print(\"Submission with plain GCS URI failed:\", e)\n",
        "\n",
        "    # Pattern 2: pass an object referencing the file (API-specific)\n",
        "    try:\n",
        "        if not model_id:\n",
        "            # Example shape: adapt to your client. Many APIs want {\"file_uri\": \"...\"} or {\"gcs_uri\": \"...\"}\n",
        "            training_dataset_ref = {\"gcs_uri\": gcs_uri}\n",
        "            tuning_op = client.tunings.tune(\n",
        "                base_model=\"models/gemini-2.5-flash\",\n",
        "                training_dataset=training_dataset_ref,\n",
        "                config=types.CreateTuningJobConfig(\n",
        "                    tuned_model_display_name=\"Newsgroup classification model\",\n",
        "                    batch_size=16,\n",
        "                    epoch_count=2,\n",
        "                ),\n",
        "            )\n",
        "            print(\"Tuning job submitted (using training_dataset object).\")\n",
        "    except Exception as e:\n",
        "        print(\"Submission with training_dataset object failed:\", e)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"GCS upload block failed. If you don't want to use GCS, skip Option B. Error:\", e)\n"
      ],
      "metadata": {
        "id": "FHITJYKLQC0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = client.models.list()\n",
        "for model in models:\n",
        "  print(model.name)"
      ],
      "metadata": {
        "id": "1PKBebcqIqFl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}